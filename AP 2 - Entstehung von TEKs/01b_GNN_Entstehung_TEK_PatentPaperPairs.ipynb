{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@pytorch_geometric/link-prediction-on-heterogeneous-graphs-with-pyg-6d5c29677c70\n",
    "https://towardsdatascience.com/graph-neural-networks-with-pyg-on-node-classification-link-prediction-and-anomaly-detection-14aa38fe1275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "# import ast\n",
    "from sqlalchemy import create_engine, URL, text\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from rapidfuzz import fuzz, process, distance\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from torch_geometric.data import HeteroData, Dataset, Data\n",
    "from torch_geometric.nn import SAGEConv, GATConv, HeteroConv, MessagePassing\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os.path as osp\n",
    "import gcld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert/distilbert-base-uncased')\n",
    "detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAlex Works from Reliance on Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs = pd.read_csv(\"/mnt/hdd01/Reliance on Science/Raw Files/_pcs_oa.csv\")\n",
    "df_rel_ppp = pd.read_csv(\"/mnt/hdd01/Reliance on Science/Raw Files/_patent_paper_pairs.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_ppp = df_rel_ppp.astype(str)\n",
    "df_rel_ppp['patent'] = df_rel_ppp['patent'].apply(lambda x: x.lower())\n",
    "df_rel_ppp = df_rel_ppp[df_rel_ppp['patent'].apply(lambda x: \"us\" in x)] # ONLY US patents in the original dataset\n",
    "df_rel_ppp['patent_id'] = df_rel_ppp['patent'].apply(lambda x: x.split(\"-\", 1)[1].rsplit(\"-\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs = df_rel_pcs.astype(str)\n",
    "df_rel_pcs['patent'] = df_rel_pcs['patent'].apply(lambda x: x.lower())\n",
    "df_rel_pcs = df_rel_pcs[df_rel_pcs['patent'].apply(lambda x: \"us\" in x)]\n",
    "df_rel_pcs['patent_id'] = df_rel_pcs['patent'].apply(lambda x: x.split(\"-\", 1)[1].rsplit(\"-\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs_filtered = df_rel_pcs[~df_rel_pcs['oaid'].isin(df_rel_ppp['magid'])]\n",
    "df_rel_pcs_filtered = df_rel_pcs_filtered.groupby('oaid').filter(lambda x: len(x) >= 5) # 5 is the minimum number of mentions in the dataset\n",
    "df_rel_pcs_filtered = df_rel_pcs_filtered.groupby('patent_id').filter(lambda x: len(x) >= 5) # 5 is the minimum number of mentions in the dataset\n",
    "df_rel_pcs_sample = df_rel_pcs_filtered.sample(n=df_rel_ppp['magid'].nunique(), random_state=42)\n",
    "df_rel_pcs_sample = df_rel_pcs_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "oaid_list = np.unique(np.concatenate([df_rel_ppp['magid'].unique(), df_rel_pcs_sample['oaid'].unique()]))\n",
    "patent_list = np.unique(np.concatenate([df_rel_ppp['patent_id'].unique(), df_rel_pcs_sample['patent_id'].unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Works from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_object = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=\"tie\",\n",
    "    password=\"\",\n",
    "    host=\"\",\n",
    "    port=\"\",\n",
    "    database=\"\",\n",
    ")\n",
    "engine = create_engine(url_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TEMPORARY TABLE temp_oaid_ppp (\n",
    "            oaid VARCHAR PRIMARY KEY\n",
    "        )\n",
    "    \"\"\"))\n",
    "    oaid_prefixed = ['https://openalex.org/W' + str(oaid) for oaid in oaid_list]\n",
    "    for oaid in tqdm(oaid_prefixed):\n",
    "        connection.execute(text(\"INSERT INTO temp_oaid_ppp (oaid) VALUES (:oaid)\"), {'oaid': oaid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT w.id, w.title, w.abstract_inverted_index \n",
    "    FROM openalex.works AS w\n",
    "    JOIN temp_oaid_ppp AS t ON w.id = t.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_postgres = df_rel_postgres.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = df_rel_postgres[df_rel_postgres['abstract_inverted_index'].apply(lambda x: x is not None and x['InvertedIndex'] != {})]\n",
    "\n",
    "def reconstruct_abstract(row):\n",
    "    # Extract the InvertedIndex for the current row\n",
    "    inverted_index = row['abstract_inverted_index']['InvertedIndex']\n",
    "    \n",
    "    # Create a mapping of positions to words\n",
    "    position_to_word = {}\n",
    "    for word, positions in inverted_index.items():\n",
    "        for position in positions:\n",
    "            position_to_word[position] = word\n",
    "    \n",
    "    # Sort positions and reconstruct the abstract\n",
    "    sorted_positions = sorted(position_to_word.keys())\n",
    "    full_text_abstract = \" \".join(position_to_word[pos] for pos in sorted_positions)\n",
    "    \n",
    "    # Fix punctuation spacing\n",
    "    full_text_abstract = re.sub(r'\\s+([.,;?!:])', r'\\1', full_text_abstract)\n",
    "    \n",
    "    return full_text_abstract\n",
    "\n",
    "df_rel_postgres['abstract'] = df_rel_postgres.apply(reconstruct_abstract, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_works.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_works.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Authors from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT a.work_id, a.author_id\n",
    "    FROM openalex.works_authorships as a\n",
    "    JOIN temp_oaid_ppp AS t ON a.work_id = t.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_authors_postgres = df_rel_authors_postgres.drop_duplicates(subset=['author_id', 'work_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_postgres_grouped = df_rel_authors_postgres.groupby('author_id')['work_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TEMPORARY TABLE temp_author_id_ppp (\n",
    "            author_id VARCHAR PRIMARY KEY\n",
    "        )\n",
    "    \"\"\"))\n",
    "    for author_id in tqdm(df_rel_authors_postgres_grouped['author_id']):\n",
    "        connection.execute(text(\"INSERT INTO temp_author_id_ppp (author_id) VALUES (:author_id)\"), {'author_id': author_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_info_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT a.id, a.display_name, a.display_name_alternatives\n",
    "    FROM openalex.authors as a\n",
    "    JOIN temp_author_id_ppp AS t ON a.id = t.author_id\n",
    "\"\"\", con=engine)\n",
    "df_rel_authors_info_postgres = df_rel_authors_info_postgres.drop_duplicates(subset=['display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete = pd.merge(df_rel_authors_postgres_grouped, df_rel_authors_info_postgres, left_on='author_id', right_on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete['oaid'] = df_rel_authors_complete['work_id'].apply(lambda x: [i.replace(\"https://openalex.org/W\", \"\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel_authors_complete = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_authors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Paper Citations from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT w.work_id, w.referenced_work_id\n",
    "    FROM openalex.works_referenced_works as w\n",
    "    JOIN temp_oaid_ppp AS t1 ON w.work_id = t1.oaid\n",
    "    JOIN temp_oaid_ppp AS t2 ON w.referenced_work_id = t2.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_citations_postgres = df_rel_citations_postgres.drop_duplicates(subset=['work_id', 'referenced_work_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_citations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel_citations_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_citations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Patents from PATSTAT Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_object = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=\"tie\",\n",
    "    password=\"\",\n",
    "    host=\"\",\n",
    "    port=\"\",\n",
    "    database=\"\",\n",
    ")\n",
    "engine = create_engine(url_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TEMPORARY TABLE temp_patentid_ppp (\n",
    "            patentid VARCHAR PRIMARY KEY\n",
    "        )\n",
    "    \"\"\"))\n",
    "    for patent_id in tqdm(patent_list):\n",
    "        connection.execute(text(\"INSERT INTO temp_patentid_ppp (patentid) VALUES (:patentid)\"), {'patentid': patent_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching of Authors, Inventor for Patents with Authors for Papers (PCS and PPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_authors = df_patent_authors[df_patent_authors['full_name'] != \"nan nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete['display_name_alternatives'] = df_rel_authors_complete['display_name_alternatives'].apply(lambda x: ast.literal_eval(x))\n",
    "df_rel_authors_complete['display_name_alternatives'] = df_rel_authors_complete.apply(lambda row: row['display_name_alternatives'] + [row['display_name']] if isinstance(row['display_name_alternatives'], list) else [row['display_name']], axis=1)\n",
    "df_rel_authors_complete_exploded = df_rel_authors_complete.explode('display_name_alternatives')\n",
    "df_rel_authors_complete_exploded['oaid'] = df_rel_authors_complete_exploded['work_id'].apply(lambda x: [i.replace(\"https://openalex.org/W\", \"\") for i in eval(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete_exploded['display_name_alternatives'] = df_rel_authors_complete_exploded['display_name_alternatives'].apply(lambda x: x.lower())\n",
    "df_patent_authors['full_name'] = df_patent_authors['full_name'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the match_names function\n",
    "# def match_names(index):\n",
    "#     row = df_patent_authors.loc[index]\n",
    "#     full_name = row.get('full_name')\n",
    "#     if full_name is not None:\n",
    "#         return process.extractOne(full_name, df_rel_authors_complete_exploded['display_name_alternatives'], scorer=fuzz.token_set_ratio)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Use ProcessPoolExecutor to parallelize the matching process\n",
    "# with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:\n",
    "#     futures = {executor.submit(match_names, index) for index in df_patent_authors.index}\n",
    "#     results = []\n",
    "#     for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#         results.append(future.result())\n",
    "#     df_patent_authors['best_match'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching depending on pcs and ppp relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete_exploded_exploded = df_rel_authors_complete_exploded.explode('oaid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_authors_filtered = df_patent_authors[df_patent_authors['full_name'].notna()]\n",
    "df_merged = pd.merge(df_patent_authors_filtered, df_rel, on='patent_id')\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_rel_authors_complete_exploded_exploded, on='oaid')\n",
    "df_merged = df_merged[['patent_id', 'assignee_id', 'inventor_id', 'full_name', 'oaid', 'author_id', 'display_name', 'display_name_alternatives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_names(row):\n",
    "    full_name = row['full_name']\n",
    "    match = distance.Levenshtein.normalized_similarity(full_name, row['display_name_alternatives'])\n",
    "    return match\n",
    "\n",
    "df_merged['best_match'] = df_merged.progress_apply(match_names, axis=1)\n",
    "# df_merged_test['best_match'] = df_merged_test.progress_apply(match_names, axis=1)\n",
    "\n",
    "# df_merged = df_merged.sort_values('best_match', ascending=False)\n",
    "df_merged_filtered = df_merged[df_merged['best_match'] >= 0.75]\n",
    "df_merged_filtered = df_merged_filtered.loc[df_merged_filtered.groupby(['patent_id', 'assignee_id', 'inventor_id', 'oaid', 'author_id'])['best_match'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct final authors dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_authors_filtered = df_patent_authors[~df_patent_authors[['patent_id', 'assignee_id', 'inventor_id']].apply(tuple, 1).isin(df_merged_filtered[['patent_id', 'assignee_id', 'inventor_id']].apply(tuple, 1))]\n",
    "df_patent_authors_filtered = df_patent_authors_filtered.drop_duplicates(subset=['patent_id', 'assignee_id', 'inventor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete_filtered = df_rel_authors_complete_exploded_exploded[~df_rel_authors_complete_exploded_exploded[['oaid', 'author_id']].apply(tuple, 1).isin(df_merged_filtered[['oaid', 'author_id']].apply(tuple, 1))]\n",
    "df_rel_authors_complete_filtered = df_rel_authors_complete_filtered.drop_duplicates(subset=['oaid', 'author_id'])\n",
    "df_rel_authors_complete_filtered = df_rel_authors_complete_filtered[['oaid', 'author_id', 'display_name', 'display_name_alternatives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.concat([df_merged_filtered, df_patent_authors_filtered, df_rel_authors_complete_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.to_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_patentsview_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_authors = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_patentsview_authors.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Preparation - Embedding of all properties; Edge Indices for all properties; Create H5PY files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding of Node Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_cpc['embedding'] = model.encode(df_patent_cpc['patent_title'] + ' [SEP] ' + df_patent_cpc['patent_abstract'].apply(lambda x: \" \".join(x)), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres['embedding'] = model.encode(df_rel_postgres['title'] + ' [SEP] ' + df_rel_postgres['abstract'].apply(lambda x: \" \".join(x)), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_cpc.to_csv(\"/mnt/hdd01/Reliance on Science/cleantech_patents_embedding.csv\", index=False)\n",
    "df_rel_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_works_embedding.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patent_cpc = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_patents_embedding.csv\", dtype=str)\n",
    "# df_rel_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/cleantech_oa_works_embedding.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model.get_sentence_embedding_dimension()  \n",
    "df_authors['embedding'] = df_authors.apply(lambda _: np.random.rand(d), axis=1)\n",
    "df_authors['embedding'] = df_authors['embedding'].apply(lambda x: x / np.linalg.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Indices for all relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_to_index = pd.Series(df_patent_cpc.index, index=df_patent_cpc['patent_id']).to_dict()\n",
    "df_patent_edge_index = df_patent_citations.copy()\n",
    "df_patent_edge_index = df_patent_edge_index[['patent_id', 'citation_patent_id']]\n",
    "df_patent_edge_index['patent_id'] = df_patent_edge_index['patent_id'].map(patent_id_to_index)\n",
    "df_patent_edge_index['citation_patent_id'] = df_patent_edge_index['citation_patent_id'].map(patent_id_to_index)\n",
    "df_patent_edge_index = df_patent_edge_index.drop_duplicates(subset=['patent_id', 'citation_patent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres['work_id'] = df_rel_citations_postgres['work_id'].apply(lambda x: x.replace(\"https://openalex.org/W\", \"\"))\n",
    "df_rel_citations_postgres['referenced_work_id'] = df_rel_citations_postgres['referenced_work_id'].apply(lambda x: x.replace(\"https://openalex.org/W\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id_to_index = pd.Series(df_rel_postgres.index, index=df_rel_postgres['oaid']).to_dict()\n",
    "df_paper_edge_index = df_rel_citations_postgres.copy()\n",
    "df_paper_edge_index = df_paper_edge_index[['work_id', 'referenced_work_id']]\n",
    "df_paper_edge_index['work_id'] = df_paper_edge_index['work_id'].map(paper_id_to_index)\n",
    "df_paper_edge_index['referenced_work_id'] = df_paper_edge_index['referenced_work_id'].map(paper_id_to_index)\n",
    "df_paper_edge_index = df_paper_edge_index.drop_duplicates(subset=['work_id', 'referenced_work_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_paper_edge_index = df_rel.copy()\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index[['patent_id', 'oaid']]\n",
    "df_patent_paper_edge_index['patent_id'] = df_patent_paper_edge_index['patent_id'].map(patent_id_to_index)\n",
    "df_patent_paper_edge_index['oaid'] = df_patent_paper_edge_index['oaid'].map(paper_id_to_index)\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index.drop_duplicates(subset=['patent_id', 'oaid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_id_to_index = pd.Series(df_authors.index, index=df_authors['author_id']).to_dict()\n",
    "df_author_patent_edge_index = df_authors.copy()\n",
    "# df_author_patent_edge_index = df_author_patent_edge_index.astype(str)\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[['author_id', 'patent_id']]\n",
    "df_author_patent_edge_index['author_id'] = df_author_patent_edge_index['author_id'].map(author_id_to_index)\n",
    "df_author_patent_edge_index['patent_id'] = df_author_patent_edge_index['patent_id'].map(patent_id_to_index)\n",
    "df_patent_author_edge_index = df_author_patent_edge_index[['patent_id', 'author_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_author_paper_edge_index = df_authors.copy()\n",
    "# df_author_paper_edge_index = df_author_paper_edge_index.astype(str)\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[['author_id', 'oaid']]\n",
    "df_author_paper_edge_index['author_id'] = df_author_paper_edge_index['author_id'].map(author_id_to_index)\n",
    "df_author_paper_edge_index['oaid'] = df_author_paper_edge_index['oaid'].map(paper_id_to_index)\n",
    "df_paper_author_edge_index = df_author_paper_edge_index[['oaid', 'author_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_author_patent_edge_index = df_author_patent_edge_index.drop_duplicates(subset=['author_id', 'patent_id'])\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.drop_duplicates(subset=['author_id', 'oaid'])\n",
    "df_patent_author_edge_index = df_patent_author_edge_index.drop_duplicates(subset=['patent_id', 'author_id'])\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.drop_duplicates(subset=['oaid', 'author_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create H5PY files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where strings are \"nan\"\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['patent_id'] != \"nan\"]\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['author_id'] != \"nan\"]\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['oaid'] != \"nan\"]\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['author_id'] != \"nan\"]\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['author_id'] != \"nan\"]\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['patent_id'] != \"nan\"]\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['author_id'] != \"nan\"]\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['oaid'] != \"nan\"]\n",
    "\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['patent_id'] != \"nan\"]\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['citation_patent_id'] != \"nan\"]\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['work_id'] != \"nan\"]\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['referenced_work_id'] != \"nan\"]\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['patent_id'] != \"nan\"]\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['oaid'] != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_edge_index = df_patent_edge_index.astype(int)\n",
    "df_paper_edge_index = df_paper_edge_index.astype(int)\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index.astype(int)\n",
    "df_author_patent_edge_index = df_author_patent_edge_index.astype(int)\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.astype(int)\n",
    "df_patent_author_edge_index = df_patent_author_edge_index.astype(int)\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(str_repr):\n",
    "    return np.fromstring(str_repr.strip('[]'), sep=',')\n",
    "\n",
    "# df_patent_cpc[\"embedding\"] = df_patent_cpc[\"embedding\"].apply(string_to_array)\n",
    "# df_rel_postgres[\"embedding\"] = df_rel_postgres[\"embedding\"].apply(string_to_array)\n",
    "# df_rel_postgres[\"patent_paper_pair\"] = df_rel_postgres[\"patent_paper_pair\"].astype(int)\n",
    "\n",
    "# Delete all NaN values from edge indices\n",
    "df_patent_edge_index = df_patent_edge_index.dropna()\n",
    "df_paper_edge_index = df_paper_edge_index.dropna()\n",
    "df_patent_paper_edge_index = df_patent_paper_edge_index.dropna()\n",
    "df_author_patent_edge_index = df_author_patent_edge_index.dropna()\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.dropna()\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.dropna()\n",
    "\n",
    "# Open an HDF5 file\n",
    "with h5py.File('/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK Emergence/raw/torch_tek_dataset_distilbert_emergence.h5', 'w') as f:\n",
    "    # Save node data\n",
    "    f.create_dataset('g_patent/x', data=np.stack(df_patent_cpc[\"embedding\"].values))\n",
    "    f.create_dataset('g_paper/x', data=np.stack(df_rel_postgres[\"embedding\"].values))\n",
    "    f.create_dataset('g_paper/y', data=np.stack(df_rel_postgres[\"patent_paper_pair\"].values))\n",
    "    f.create_dataset('g_author/x', data=np.stack(df_authors[\"embedding\"].values))\n",
    "    \n",
    "    # Save edge indices\n",
    "    f.create_dataset('patent_edge_index', data=df_patent_edge_index.values, dtype=np.int64)\n",
    "    f.create_dataset('paper_edge_index', data=df_paper_edge_index.values, dtype=np.int64)\n",
    "    f.create_dataset('patent_paper_edge_index', data=df_patent_paper_edge_index.values, dtype=np.int64)\n",
    "    f.create_dataset('author_patent_edge_index', data=df_author_patent_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('patent_author_edge_index', data=df_patent_author_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('author_paper_edge_index', data=df_author_paper_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('paper_author_edge_index', data=df_paper_author_edge_index, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Heterogeneous Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPPHeteroDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PPPHeteroDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        # processed_path = osp.join(self.processed_dir, self.processed_file_names)\n",
    "        # if osp.exists(processed_path):\n",
    "        #     self.data = torch.load(processed_path)\n",
    "        # else:\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return '/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK Emergence/raw/'\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return '/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK Emergence/processed/'\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\n",
    "            'torch_tek_dataset_distilbert_emergence.h5'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'gnn_tek_data_distilbert_emergence.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Initialize HeteroData object\n",
    "        data = HeteroData()\n",
    "    \n",
    "        # Open an HDF5 file\n",
    "        with h5py.File(osp.join(self.raw_dir, 'torch_tek_dataset_distilbert_emergence.h5'), 'r') as f:\n",
    "            # Load and process node features\n",
    "            data['patent'].x = torch.tensor(f['g_patent/x'][:], dtype=torch.float)\n",
    "            data['paper'].x = torch.tensor(f['g_paper/x'][:], dtype=torch.float)\n",
    "            data['paper'].y = torch.tensor(f['g_paper/y'][:], dtype=torch.long)\n",
    "\n",
    "            data['author'].x = torch.tensor(f['g_author/x'][:], dtype=torch.float)\n",
    "            \n",
    "            # Load and process edge indices\n",
    "            data['patent', 'cites', 'patent'].edge_index = torch.tensor(f['patent_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['paper', 'cites', 'paper'].edge_index = torch.tensor(f['paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['patent', 'cites', 'paper'].edge_index = torch.tensor(f['patent_paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "\n",
    "            data['author', 'author_of_patent', 'patent'].edge_index = torch.tensor(f['author_patent_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['author', 'author_of_paper', 'paper'].edge_index = torch.tensor(f['author_paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['patent', 'has_author_patent', 'author'].edge_index = torch.tensor(f['patent_author_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['paper', 'has_author_paper', 'author'].edge_index = torch.tensor(f['paper_author_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        # Create train_mask, val_mask, and test_mask\n",
    "        data['paper'].train_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        # data['paper'].val_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        data['paper'].test_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        data['paper'].train_mask[:int(0.8*data['paper'].num_nodes)] = 1\n",
    "        # data['paper'].val_mask[int(0.8*data['paper'].num_nodes):int(0.9*data['paper'].num_nodes)] = 1]\n",
    "        data['paper'].test_mask[int(0.8*data['paper'].num_nodes):] = 1\n",
    "\n",
    "        # Diagnostic print statements\n",
    "        print(\"Data keys after processing:\", data.keys())\n",
    "        print(\"Node types and their feature shapes:\")\n",
    "        for node_type, node_data in data.node_items():\n",
    "            print(f\"Node type: {node_type}\")\n",
    "            for key, item in node_data.items():\n",
    "                if key == 'x' or key == 'y':\n",
    "                    print(f\"Features ({key}) shape:\", item.size())\n",
    "\n",
    "        print(\"Edge types and their index shapes:\")\n",
    "        for edge_type, edge_data in data.edge_items():\n",
    "            print(f\"Edge type: {edge_type}\")\n",
    "            if 'edge_index' in edge_data:\n",
    "                print(\"Edge index shape:\", edge_data['edge_index'].size())\n",
    "            else:\n",
    "                print(f\"{edge_type} has no edge index.\")\n",
    "        \n",
    "\n",
    "        self.data = data  # Save the processed data to self.data\n",
    "        torch.save(data, osp.join(self.processed_dir, self.processed_file_names))\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_dataset = PPPHeteroDataset(root='/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK Emergence/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_dataset_0 = ppp_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels of the paper nodes in the train and test set\n",
    "train_labels = ppp_dataset_0['paper'].y[ppp_dataset_0['paper'].train_mask].cpu().numpy()\n",
    "test_labels = ppp_dataset_0['paper'].y[ppp_dataset_0['paper'].test_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value counts of the labels\n",
    "print(\"Value counts of labels for paper nodes in train set:\", np.bincount(train_labels))\n",
    "print(\"Value counts of labels for paper nodes in test set:\", np.bincount(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGCN(MessagePassing):\n",
    "    def __init__(self, hidden_channels, num_node_features_dict, num_classes):\n",
    "        super(HeteroGCN, self).__init__(aggr='mean')\n",
    "        torch.manual_seed(42) # For reproducible results\n",
    "        \n",
    "        # Define a separate SAGEConv for each edge type with correct input feature sizes\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('patent', 'cites', 'patent'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'cites', 'paper'): SAGEConv(num_node_features_dict['paper'], hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'cites', 'paper'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_patent', 'patent'): SAGEConv(num_node_features_dict['author'], hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_paper', 'paper'): SAGEConv(num_node_features_dict['author'], hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'has_author_patent', 'author'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'has_author_paper', 'author'): SAGEConv(num_node_features_dict['paper'], hidden_channels, add_self_loops=True)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('patent', 'cites', 'patent'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'cites', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'cites', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_patent', 'patent'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_paper', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'has_author_patent', 'author'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'has_author_paper', 'author'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Linear layer for classifying patents\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict, edge_index_dict = data.x_dict, data.edge_index_dict\n",
    "\n",
    "        # Include dropout for regularization\n",
    "        # x_dict['paper'] = F.dropout(x_dict['paper'], p=0.2, training=self.training)\n",
    "\n",
    "        # First convolution layer\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        # Second convolution layer\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        # Only use the 'patent' node embeddings for the final prediction\n",
    "        out = self.lin(x_dict['paper'])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features_dict = {'patent': 768, 'paper': 768, 'author': 768}\n",
    "model = HeteroGCN(hidden_channels=64, num_node_features_dict=num_node_features_dict, num_classes=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbor Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "ppp_dataset_0 = ppp_dataset[0].to(device)\n",
    "train_loader = NeighborLoader(ppp_dataset_0, num_neighbors=[100], batch_size=512, shuffle=True, input_nodes=('paper', ppp_dataset_0['paper'].train_mask))\n",
    "test_loader = NeighborLoader(ppp_dataset_0, num_neighbors=[100], batch_size=512, shuffle=False, input_nodes=('paper', ppp_dataset_0['paper'].test_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            out = model(batch)\n",
    "            loss = criterion(out[batch['paper'].train_mask], batch['paper'].y[batch['paper'].train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        except Exception as e:\n",
    "            print(\"Error during training:\", e)\n",
    "            raise\n",
    "            \n",
    "    return total_loss / total_batches if total_batches else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "\n",
    "            pred = out.argmax(dim=1)\n",
    "            \n",
    "            test_mask = batch['paper'].test_mask # For Hetereogeneous NN \n",
    "            test_labels = batch['paper'].y # For Hetereogeneous NN \n",
    "\n",
    "            # Update correct and total counts\n",
    "            correct += int((pred[test_mask] == test_labels[test_mask]).sum())\n",
    "            total += int(test_mask.sum())\n",
    "\n",
    "    test_acc = correct / total\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train()\n",
    "    test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
