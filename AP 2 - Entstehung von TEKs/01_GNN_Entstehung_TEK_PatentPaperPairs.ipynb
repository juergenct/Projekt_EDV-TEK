{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Emergence of TEK - Patent Paper Pairs with Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import ast\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "import os.path as osp\n",
    "import gcld3\n",
    "from sqlalchemy import create_engine, URL, text, MetaData, Table\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from rapidfuzz import fuzz, process, distance\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData, Dataset, Data\n",
    "from torch_geometric.nn import SAGEConv, GATConv, HeteroConv, MessagePassing\n",
    "from torch_geometric.loader import NeighborLoader, DataLoader, LinkNeighborLoader\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/distilbert_distilbert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert/distilbert-base-uncased')\n",
    "detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAlex Works from Reliance on Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs = pd.read_csv(\"/mnt/hdd01/Reliance on Science/Raw Files/_pcs_oa.csv\")\n",
    "df_rel_ppp = pd.read_csv(\"/mnt/hdd01/Reliance on Science/Raw Files/_patent_paper_pairs.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_ppp = df_rel_ppp.astype(str)\n",
    "df_rel_ppp['patent'] = df_rel_ppp['patent'].apply(lambda x: x.lower())\n",
    "df_rel_ppp = df_rel_ppp[df_rel_ppp['patent'].apply(lambda x: \"us\" in x)] # ONLY US patents in the original dataset - inference on US and EP patents!!\n",
    "df_rel_ppp['patent_id'] = df_rel_ppp['patent'].apply(lambda x: x.split(\"-\", 1)[1].rsplit(\"-\", 1)[0])\n",
    "df_rel_ppp['patent_paper_pair'] = 1\n",
    "df_rel_ppp = df_rel_ppp.rename(columns={'magid': 'oaid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs = df_rel_pcs.astype(str)\n",
    "df_rel_pcs['patent'] = df_rel_pcs['patent'].apply(lambda x: x.lower())\n",
    "df_rel_pcs = df_rel_pcs[df_rel_pcs['patent'].apply(lambda x: \"us\" in x)]\n",
    "df_rel_pcs['patent_id'] = df_rel_pcs['patent'].apply(lambda x: x.split(\"-\", 1)[1].rsplit(\"-\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_pcs_filtered = df_rel_pcs[~df_rel_pcs['oaid'].isin(df_rel_ppp['oaid'])]\n",
    "df_rel_pcs_filtered = df_rel_pcs_filtered.groupby('oaid').filter(lambda x: len(x) >= 5) # 5 is the minimum number of mentions in the dataset, to ensure well connected graph\n",
    "df_rel_pcs_filtered = df_rel_pcs_filtered.groupby('patent_id').filter(lambda x: len(x) >= 5) # 5 is the minimum number of mentions in the dataset, to ensure well connected graph\n",
    "df_rel_pcs_sample = df_rel_pcs_filtered.sample(n=df_rel_ppp['oaid'].nunique(), random_state=42)\n",
    "df_rel_pcs_sample = df_rel_pcs_sample.reset_index(drop=True)\n",
    "df_rel_pcs_sample['patent_paper_pair'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oaid_list = np.unique(np.concatenate([df_rel_ppp['oaid'].unique(), df_rel_pcs_sample['oaid'].unique()]))\n",
    "patent_list = np.unique(np.concatenate([df_rel_ppp['patent_id'].unique(), df_rel_pcs_sample['patent_id'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_info = pd.concat([df_rel_ppp, df_rel_pcs_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Works from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_object = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=user,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port,\n",
    "    database=db,\n",
    ")\n",
    "engine = create_engine(url_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TABLE temp_oaid_ppp (\n",
    "            oaid VARCHAR PRIMARY KEY\n",
    "        )\n",
    "    \"\"\"))\n",
    "    oaid_prefixed = ['https://openalex.org/W' + str(oaid) for oaid in oaid_list]\n",
    "    for oaid in tqdm(oaid_prefixed):\n",
    "        connection.execute(text(\"INSERT INTO temp_oaid_ppp (oaid) VALUES (:oaid)\"), {'oaid': oaid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT w.id, w.title, w.abstract_inverted_index \n",
    "    FROM openalex.works AS w\n",
    "    JOIN temp_oaid_ppp AS t ON w.id = t.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_postgres = df_rel_postgres.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = df_rel_postgres[df_rel_postgres['abstract_inverted_index'].apply(lambda x: x is not None and x['InvertedIndex'] != {})]\n",
    "\n",
    "def reconstruct_abstract(row):\n",
    "    # Extract the InvertedIndex for the current row\n",
    "    inverted_index = row['abstract_inverted_index']['InvertedIndex']\n",
    "    \n",
    "    # Create a mapping of positions to words\n",
    "    position_to_word = {}\n",
    "    for word, positions in inverted_index.items():\n",
    "        for position in positions:\n",
    "            position_to_word[position] = word\n",
    "    \n",
    "    # Sort positions and reconstruct the abstract\n",
    "    sorted_positions = sorted(position_to_word.keys())\n",
    "    full_text_abstract = \" \".join(position_to_word[pos] for pos in sorted_positions)\n",
    "    \n",
    "    # Fix punctuation spacing\n",
    "    full_text_abstract = re.sub(r'\\s+([.,;?!:])', r'\\1', full_text_abstract)\n",
    "    \n",
    "    return full_text_abstract\n",
    "\n",
    "df_rel_postgres['abstract'] = df_rel_postgres.apply(reconstruct_abstract, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_works.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_works.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Authors from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT a.work_id, a.author_id\n",
    "    FROM openalex.works_authorships as a\n",
    "    JOIN temp_oaid_ppp AS t ON a.work_id = t.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_authors_postgres = df_rel_authors_postgres.drop_duplicates(subset=['author_id', 'work_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_postgres_grouped = df_rel_authors_postgres.groupby('author_id')['work_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TEMPORARY TABLE temp_author_id_ppp (\n",
    "            author_id VARCHAR PRIMARY KEY\n",
    "        )\n",
    "    \"\"\"))\n",
    "    for author_id in tqdm(df_rel_authors_postgres_grouped['author_id']):\n",
    "        connection.execute(text(\"INSERT INTO temp_author_id_ppp (author_id) VALUES (:author_id)\"), {'author_id': author_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_info_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT a.id, a.display_name, a.display_name_alternatives\n",
    "    FROM openalex.authors as a\n",
    "    JOIN temp_author_id_ppp AS t ON a.id = t.author_id\n",
    "\"\"\", con=engine)\n",
    "df_rel_authors_info_postgres = df_rel_authors_info_postgres.drop_duplicates(subset=['display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete = pd.merge(df_rel_authors_postgres_grouped, df_rel_authors_info_postgres, left_on='author_id', right_on='id', how='inner')\n",
    "df_rel_authors_complete['oaid'] = df_rel_authors_complete['work_id'].apply(lambda x: [i.replace(\"https://openalex.org/W\", \"\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_authors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Paper Citations from Postgres OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres = pd.read_sql_query(\"\"\"\n",
    "    SELECT w.work_id, w.referenced_work_id\n",
    "    FROM openalex.works_referenced_works as w\n",
    "    JOIN temp_oaid_ppp AS t1 ON w.work_id = t1.oaid\n",
    "    JOIN temp_oaid_ppp AS t2 ON w.referenced_work_id = t2.oaid\n",
    "\"\"\", con=engine)\n",
    "df_rel_citations_postgres = df_rel_citations_postgres.drop_duplicates(subset=['work_id', 'referenced_work_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_citations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_citations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Patents from PATSTAT Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_object = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=user,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port,\n",
    "    database=db,\n",
    ")\n",
    "engine = create_engine(url_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        CREATE TABLE temp_patentid_ppp (\n",
    "            patentid VARCHAR PRIMARY KEY,\n",
    "            appln_id VARCHAR\n",
    "        )\n",
    "    \"\"\"))\n",
    "    for patent_id in tqdm(patent_list):\n",
    "        connection.execute(text(\"INSERT INTO temp_patentid_ppp (patentid) VALUES (:patentid)\"), {'patentid': patent_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract appln_id for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_applnid = pd.read_sql_query(\"\"\"\n",
    "    SELECT t.publn_nr, t.appln_id\n",
    "    FROM tls211_pat_publn as t\n",
    "    JOIN temp_patentid_ppp AS tp ON t.publn_nr = tp.patentid\n",
    "    WHERE t.publn_auth = 'US'\n",
    "\"\"\", con=engine)\n",
    "df_patstat_applnid = df_patstat_applnid.drop_duplicates(subset=['publn_nr', 'appln_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "        UPDATE temp_patentid_ppp\n",
    "        SET appln_id = t.appln_id\n",
    "        FROM (\n",
    "            SELECT t.publn_nr, t.appln_id\n",
    "            FROM tls211_pat_publn as t\n",
    "            WHERE t.publn_auth = 'US'\n",
    "        ) AS t\n",
    "        WHERE temp_patentid_ppp.patentid = t.publn_nr\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Cleantech Patents for later subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_cleantech = pd.read_sql_query(\"\"\"\n",
    "    SELECT t.appln_id, t.cpc_class_symbol\n",
    "    FROM tls224_appln_cpc as t\n",
    "    JOIN temp_patentid_ppp AS tp ON t.appln_id = tp.appln_id\n",
    "    WHERE t.cpc_class_symbol LIKE '%%Y02%%'\n",
    "\"\"\", con=engine)\n",
    "df_patstat_cleantech = df_patstat_cleantech.drop_duplicates(subset=['appln_id', 'cpc_class_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_cleantech = pd.merge(df_patstat_cleantech, df_patstat_applnid, on='appln_id')\n",
    "df_patstat_cleantech = df_patstat_cleantech.rename(columns={'publn_nr': 'patent_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_cleantech = pd.merge(df_rel_info, df_patstat_cleantech, on='patent_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Title from PATSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_title = pd.read_sql_query(\"\"\"\n",
    "    SELECT tp.appln_id, tp.patentid, tat.appln_title\n",
    "    FROM temp_patentid_ppp AS tp\n",
    "    JOIN tls202_appln_title AS tat ON tp.appln_id = tat.appln_id\n",
    "    WHERE tat.appln_title_lg = 'en'\n",
    "\"\"\", con=engine)\n",
    "df_patstat_title = df_patstat_title.drop_duplicates(subset=['appln_id', 'patentid', 'appln_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Abstract from PATSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_abstract = pd.read_sql_query(\"\"\"\n",
    "    SELECT tp.appln_id, tp.patentid, tab.appln_abstract\n",
    "    FROM temp_patentid_ppp AS tp\n",
    "    JOIN tls203_appln_abstr AS tab ON tp.appln_id = tab.appln_id\n",
    "    WHERE tab.appln_abstract_lg = 'en'\n",
    "\"\"\", con=engine)\n",
    "df_patstat_abstract = df_patstat_abstract.drop_duplicates(subset=['appln_id', 'patentid', 'appln_abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Authors from PATSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_id = pd.read_sql_query(\"\"\"\n",
    "    SELECT tp.appln_id, pa.person_id\n",
    "    FROM temp_patentid_ppp AS tp\n",
    "    JOIN tls207_pers_appln AS pa ON tp.appln_id = pa.appln_id\n",
    "\"\"\", con=engine)\n",
    "df_patstat_person_id = df_patstat_person_id.drop_duplicates(subset=['appln_id', 'person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_id.to_sql('temp_ppp_person_id', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_details = pd.read_sql_query(\"\"\"\n",
    "    SELECT tpi.appln_id, tpi.person_id, p.person_name, p.person_name_orig_lg, p.person_address, p.doc_std_name, p.psn_name, p.han_name\n",
    "    FROM temp_ppp_person_id AS tpi\n",
    "    JOIN tls206_person AS p ON tpi.person_id = p.person_id\n",
    "\"\"\", con=engine)\n",
    "df_patstat_person_details = df_patstat_person_details.drop_duplicates(subset=['appln_id', 'person_id', 'person_name', 'person_name_orig_lg', 'person_address', 'doc_std_name', 'psn_name', 'han_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_title.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_title.csv\", index=False)\n",
    "df_patstat_abstract.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_abstract.csv\", index=False)\n",
    "df_patstat_person_details.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_person_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_title = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_title.csv\")\n",
    "df_patstat_abstract = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_abstract.csv\")\n",
    "df_patstat_person_details = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_person_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Citations from PATSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_citations = pd.read_sql_query(\"\"\"\n",
    "    SELECT c.pat_publn_id::text, c.cited_pat_publn_id::text, p.appln_id::text\n",
    "    FROM tls212_citation AS c\n",
    "    JOIN tls211_pat_publn AS p ON c.pat_publn_id = p.pat_publn_id\n",
    "    WHERE p.appln_id IN (SELECT appln_id FROM temp_patentid_ppp)\n",
    "\"\"\", con=engine)\n",
    "df_patstat_citations = df_patstat_citations.drop_duplicates(subset=['pat_publn_id', 'cited_pat_publn_id', 'appln_id'])\n",
    "df_patstat_citations = df_patstat_citations[df_patstat_citations['cited_pat_publn_id'].isin(df_patstat_citations['pat_publn_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_citations = df_patstat_citations.rename(columns={\"appln_id\": \"pat_appln_id\"})\n",
    "df_patstat_citations = pd.merge(df_patstat_citations, df_patstat_citations[['pat_publn_id', 'pat_appln_id']].rename(columns={'pat_appln_id': 'cited_pat_appln_id'}), left_on='cited_pat_publn_id', right_on='pat_publn_id', how='inner')\n",
    "df_patstat_citations = df_patstat_citations[['pat_publn_id_x', 'cited_pat_publn_id', 'pat_appln_id', 'cited_pat_appln_id']]\n",
    "df_patstat_citations = df_patstat_citations.rename(columns={'pat_publn_id_x': 'pat_publn_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_citations.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_citations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_citations = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_citations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching of Authors, Inventor for Patents with Authors for Papers (PCS and PPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete['display_name_alternatives'] = df_rel_authors_complete['display_name_alternatives'].apply(lambda x: ast.literal_eval(x))\n",
    "df_rel_authors_complete['display_name_alternatives'] = df_rel_authors_complete.apply(lambda row: row['display_name_alternatives'] + [row['display_name']] if isinstance(row['display_name_alternatives'], list) else [row['display_name']], axis=1)\n",
    "df_rel_authors_complete_exploded = df_rel_authors_complete.explode('display_name_alternatives')\n",
    "df_rel_authors_complete_exploded['oaid'] = df_rel_authors_complete_exploded['work_id'].apply(lambda x: [i.replace(\"https://openalex.org/W\", \"\") for i in eval(x)])\n",
    "df_rel_authors_complete_exploded['display_name_alternatives'] = df_rel_authors_complete_exploded['display_name_alternatives'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_details = df_patstat_person_details.drop_duplicates(subset=['appln_id', 'person_id'])\n",
    "df_patstat_person_details_melted = df_patstat_person_details.melt(id_vars=['appln_id', 'person_id'], \n",
    "                                                                  value_vars=['person_name', 'person_name_orig_lg', 'doc_std_name', 'psn_name', 'han_name'],\n",
    "                                                                  var_name='name_type', \n",
    "                                                                  value_name='name')\n",
    "df_patstat_person_details_melted['name'] = df_patstat_person_details_melted['name'].apply(lambda x: x.lower())\n",
    "df_patstat_person_details_melted = pd.merge(df_patstat_person_details_melted, df_patstat_title[['appln_id', 'patentid']], on='appln_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_info_grouped = df_rel_info.groupby('patent_id').agg({'oaid': list}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_details_melted['patentid'] = df_patstat_person_details_melted['patentid'].astype(str)\n",
    "df_rel_info_grouped['patent_id'] = df_rel_info_grouped['patent_id'].astype(str)\n",
    "df_patstat_person_details_melted = pd.merge(df_patstat_person_details_melted, df_rel_info_grouped[['oaid', 'patent_id']], left_on='patentid', right_on='patent_id', how='inner', validate='m:m')\n",
    "# df_patstat_person_details_melted = df_patstat_person_details_melted.dropna(subset=['oaid'])\n",
    "df_patstat_person_details_melted = df_patstat_person_details_melted.dropna(subset=['patentid'])\n",
    "df_patstat_person_details_exploded = df_patstat_person_details_melted.explode('oaid')\n",
    "df_patstat_person_details_exploded = df_patstat_person_details_exploded.dropna(subset=['oaid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching depending on pcs and ppp relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete_exploded_exploded = df_rel_authors_complete_exploded.explode('oaid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_rel_authors_complete_exploded_exploded, df_patstat_person_details_exploded, on='oaid', how='inner', validate='m:m')\n",
    "df_merged = df_merged[['patent_id', 'appln_id', 'person_id', 'name_type', 'name', 'oaid', 'author_id', 'display_name', 'display_name_alternatives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_names(row):\n",
    "    full_name = row['name']\n",
    "    match = distance.Levenshtein.normalized_similarity(full_name, row['display_name_alternatives'])\n",
    "    return match\n",
    "\n",
    "df_merged['best_match'] = df_merged.progress_apply(match_names, axis=1)\n",
    "# df_merged_test['best_match'] = df_merged_test.progress_apply(match_names, axis=1)\n",
    "\n",
    "# df_merged = df_merged.sort_values('best_match', ascending=False)\n",
    "df_merged_filtered = df_merged[df_merged['best_match'] >= 0.75]\n",
    "df_merged_filtered = df_merged_filtered.loc[df_merged_filtered.groupby(['patent_id', 'appln_id', 'person_id', 'oaid', 'author_id', 'display_name'])['best_match'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct final authors dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_authors_filtered = df_patstat_person_details[~df_patstat_person_details[['appln_id', 'person_id']].apply(tuple, 1).isin(df_merged_filtered[['appln_id', 'person_id']].apply(tuple, 1))]\n",
    "df_patent_authors_filtered = df_patent_authors_filtered.drop_duplicates(subset=['appln_id', 'person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_authors_filtered['appln_id'] = df_patent_authors_filtered['appln_id'].astype(str)\n",
    "df_patstat_applnid['appln_id'] = df_patstat_applnid['appln_id'].astype(str)\n",
    "df_patent_authors_filtered = pd.merge(df_patent_authors_filtered, df_patstat_applnid, on='appln_id', how='inner', validate='m:m')\n",
    "df_patent_authors_filtered = df_patent_authors_filtered.rename(columns={'publn_nr': 'patent_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_authors_complete_filtered = df_rel_authors_complete_exploded_exploded[~df_rel_authors_complete_exploded_exploded[['oaid', 'author_id']].apply(tuple, 1).isin(df_merged_filtered[['oaid', 'author_id']].apply(tuple, 1))]\n",
    "df_rel_authors_complete_filtered = df_rel_authors_complete_filtered.drop_duplicates(subset=['oaid', 'author_id'])\n",
    "df_rel_authors_complete_filtered = df_rel_authors_complete_filtered[['oaid', 'author_id', 'display_name', 'display_name_alternatives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.concat([df_merged_filtered, df_patent_authors_filtered, df_rel_authors_complete_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.drop_duplicates(subset=['appln_id', 'person_id', 'oaid', 'author_id', 'patent_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_patentsview_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_patentsview_authors.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Preparation - Embedding of all properties; Edge Indices for all properties; Create H5PY files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding of Node Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_text = pd.merge(df_patstat_title, df_patstat_abstract, on=['appln_id', 'patentid'], how='inner')\n",
    "df_patstat_text['embedding'] = model.encode(df_patstat_text['appln_title'] + ' [SEP] ' + df_patstat_text['appln_abstract'].apply(lambda x: \" \".join(x)), device=device, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres['title'] = df_rel_postgres['title'].astype(str)\n",
    "df_rel_postgres['abstract'] = df_rel_postgres['abstract'].astype(str)\n",
    "df_rel_postgres['embedding'] = model.encode(df_rel_postgres['title'] + ' [SEP] ' + df_rel_postgres['abstract'].apply(lambda x: \" \".join(x.split())), device=device, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_works_embeddings.csv\", index=False)\n",
    "df_patstat_text.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_text_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_works_embeddings.csv\")\n",
    "df_patstat_text = pd.read_csv(\"/mnt/hdd01/Reliance on Science/ppp_patstat_text_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model.get_sentence_embedding_dimension()  \n",
    "df_authors['embedding'] = df_authors.apply(lambda _: np.random.rand(d), axis=1)\n",
    "df_authors['embedding'] = df_authors['embedding'].apply(lambda x: x / np.linalg.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.to_csv(\"/mnt/hdd01/Reliance on Science/ppp_oa_patstat_authors_embeddings.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres['oaid'] = df_rel_postgres['id'].apply(lambda x: x.replace(\"https://openalex.org/W\", \"\"))\n",
    "df_rel_info = df_rel_info[df_rel_info['oaid'].isin(df_rel_postgres['oaid'])]\n",
    "df_authors['id'] = df_authors.index\n",
    "df_rel_info = df_rel_info.sort_values('patent_paper_pair', ascending=False).drop_duplicates(subset=['oaid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_citations_postgres['work_id'] = df_rel_citations_postgres['work_id'].apply(lambda x: x.replace(\"https://openalex.org/W\", \"\"))\n",
    "df_rel_citations_postgres['referenced_work_id'] = df_rel_citations_postgres['referenced_work_id'].apply(lambda x: x.replace(\"https://openalex.org/W\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract all Cleantech Entities (REL and PATSTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel_cleantech = df_rel_postgres[df_rel_postgres['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "# df_rel_info_cleantech = df_rel_info[df_rel_info['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "# df_authors_cleantech = df_authors[df_authors['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "# df_rel_citations_postgres_cleantech = df_rel_citations_postgres[df_rel_citations_postgres['work_id'].isin(df_rel_cleantech['oaid'])]\n",
    "# df_rel_citations_postgres_cleantech_ref = df_rel_citations_postgres[df_rel_citations_postgres['referenced_work_id'].isin(df_rel_cleantech['oaid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = df_rel_postgres[~df_rel_postgres['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "df_rel_info = df_rel_info[~df_rel_info['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "df_authors = df_authors[~df_authors['oaid'].isin(df_rel_cleantech['oaid'])]\n",
    "df_rel_citations_postgres = df_rel_citations_postgres[~df_rel_citations_postgres['work_id'].isin(df_rel_cleantech['oaid'])]\n",
    "df_rel_citations_postgres = df_rel_citations_postgres[~df_rel_citations_postgres['referenced_work_id'].isin(df_rel_cleantech['oaid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_citations[['pat_appln_id', 'cited_pat_appln_id']] = df_patstat_citations[['pat_appln_id', 'cited_pat_appln_id']].astype(str)\n",
    "df_patstat_citations = df_patstat_citations.dropna(subset=['pat_appln_id', 'cited_pat_appln_id'])\n",
    "df_patstat_citations = pd.merge(df_patstat_citations, df_patstat_applnid, left_on='pat_appln_id', right_on='appln_id', how='inner')\n",
    "df_patstat_citations = df_patstat_citations.rename(columns={'publn_nr': 'patent_id'})\n",
    "df_patstat_citations = pd.merge(df_patstat_citations, df_patstat_applnid, left_on='cited_pat_appln_id', right_on='appln_id', how='inner')\n",
    "df_patstat_citations = df_patstat_citations.rename(columns={'publn_nr': 'citation_patent_id'})\n",
    "df_patstat_citations = df_patstat_citations.drop_duplicates(subset=['patent_id', 'citation_patent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_applnid = df_patstat_applnid[~df_patstat_applnid['publn_nr'].isin(df_patstat_cleantech['patent_id'])]\n",
    "df_patstat_text = df_patstat_text[~df_patstat_text['patentid'].isin(df_patstat_cleantech['patent_id'])]\n",
    "df_authors = df_authors[~df_authors['patent_id'].isin(df_patstat_cleantech['patent_id'])]\n",
    "df_patstat_citations = df_patstat_citations[~df_patstat_citations['patent_id'].isin(df_patstat_cleantech['patent_id'])]\n",
    "df_patstat_citations = df_patstat_citations[~df_patstat_citations['citation_patent_id'].isin(df_patstat_cleantech['patent_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = df_rel_postgres.reset_index(drop=True)\n",
    "df_rel_info = df_rel_info.reset_index(drop=True)\n",
    "df_authors = df_authors.reset_index(drop=True)\n",
    "df_rel_citations_postgres = df_rel_citations_postgres.reset_index(drop=True)\n",
    "\n",
    "df_patstat_citations = df_patstat_citations.reset_index(drop=True)\n",
    "df_patstat_text = df_patstat_text.reset_index(drop=True)\n",
    "df_patstat_applnid = df_patstat_applnid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Indices for all relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_applnid['appln_id'] = df_patstat_applnid['appln_id'].astype(str)\n",
    "df_patstat_text['patentid'] = df_patstat_text['patentid'].astype(str)\n",
    "df_rel_info[['patent_id', 'oaid']] = df_rel_info[['patent_id', 'oaid']].astype(str)\n",
    "df_rel_postgres['oaid'] = df_rel_postgres['oaid'].astype(str)\n",
    "df_rel_citations_postgres[['work_id', 'referenced_work_id']] = df_rel_citations_postgres[['work_id', 'referenced_work_id']].astype(str)\n",
    "df_authors[['id', 'oaid', 'patent_id']] = df_authors[['id', 'oaid', 'patent_id']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_text = df_patstat_text.reset_index(drop=True)\n",
    "patent_id_to_index = pd.Series(df_patstat_text.index, index=df_patstat_text['patentid']).to_dict()\n",
    "df_patent_edge_index = df_patstat_citations.copy()\n",
    "df_patent_edge_index = df_patent_edge_index[['patent_id', 'citation_patent_id']]\n",
    "df_patent_edge_index['patent_id'] = df_patent_edge_index['patent_id'].map(patent_id_to_index)\n",
    "df_patent_edge_index['citation_patent_id'] = df_patent_edge_index['citation_patent_id'].map(patent_id_to_index)\n",
    "df_patent_edge_index = df_patent_edge_index.drop_duplicates(subset=['patent_id', 'citation_patent_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres = df_rel_postgres.reset_index(drop=True)\n",
    "paper_id_to_index = pd.Series(df_rel_postgres.index, index=df_rel_postgres['oaid']).to_dict()\n",
    "df_paper_edge_index = df_rel_citations_postgres.copy()\n",
    "df_paper_edge_index = df_paper_edge_index[['work_id', 'referenced_work_id']]\n",
    "df_paper_edge_index['work_id'] = df_paper_edge_index['work_id'].map(paper_id_to_index)\n",
    "df_paper_edge_index['referenced_work_id'] = df_paper_edge_index['referenced_work_id'].map(paper_id_to_index)\n",
    "df_paper_edge_index = df_paper_edge_index.drop_duplicates(subset=['work_id', 'referenced_work_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patent_paper_edge_index = df_rel_info.copy()\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index[['patent_id', 'oaid']]\n",
    "# df_patent_paper_edge_index['patent_id'] = df_patent_paper_edge_index['patent_id'].map(patent_id_to_index)\n",
    "# df_patent_paper_edge_index['oaid'] = df_patent_paper_edge_index['oaid'].map(paper_id_to_index)\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index.drop_duplicates(subset=['patent_id', 'oaid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_paper_edge_index_pcs = df_rel_info.copy()\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs[['patent_id', 'oaid']]\n",
    "df_patent_paper_edge_index_pcs['patent_id'] = df_patent_paper_edge_index_pcs['patent_id'].map(patent_id_to_index)\n",
    "df_patent_paper_edge_index_pcs['oaid'] = df_patent_paper_edge_index_pcs['oaid'].map(paper_id_to_index)\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs.drop_duplicates(subset=['patent_id', 'oaid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_paper_edge_index_ppp = df_rel_info[df_rel_info['patent_paper_pair'] == 1].copy()\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp[['patent_id', 'oaid']]\n",
    "df_patent_paper_edge_index_ppp['patent_id'] = df_patent_paper_edge_index_ppp['patent_id'].map(patent_id_to_index)\n",
    "df_patent_paper_edge_index_ppp['oaid'] = df_patent_paper_edge_index_ppp['oaid'].map(paper_id_to_index)\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp.drop_duplicates(subset=['patent_id', 'oaid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_authors = df_authors.drop_duplicates(subset=['patent_id', 'person_id'])\n",
    "# df_authors = df_authors.drop_duplicates(subset=['author_id', 'oaid'])\n",
    "df_authors = df_authors.reset_index(drop=True)\n",
    "df_authors['id'] = df_authors.index\n",
    "author_id_to_index = pd.Series(df_authors.index, index=df_authors['id']).to_dict()\n",
    "df_author_patent_edge_index = df_authors.copy()\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[['id', 'patent_id']]\n",
    "df_author_patent_edge_index['id'] = df_author_patent_edge_index['id'].map(author_id_to_index)\n",
    "df_author_patent_edge_index['patent_id'] = df_author_patent_edge_index['patent_id'].map(patent_id_to_index)\n",
    "df_patent_author_edge_index = df_author_patent_edge_index[['patent_id', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_author_paper_edge_index = df_authors.copy()\n",
    "# df_author_paper_edge_index = df_author_paper_edge_index.astype(str)\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[['id', 'oaid']]\n",
    "df_author_paper_edge_index['id'] = df_author_paper_edge_index['id'].map(author_id_to_index)\n",
    "df_author_paper_edge_index['oaid'] = df_author_paper_edge_index['oaid'].map(paper_id_to_index)\n",
    "df_paper_author_edge_index = df_author_paper_edge_index[['oaid', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_author_patent_edge_index = df_author_patent_edge_index.drop_duplicates(subset=['id', 'patent_id'])\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.drop_duplicates(subset=['id', 'oaid'])\n",
    "df_patent_author_edge_index = df_patent_author_edge_index.drop_duplicates(subset=['patent_id', 'id'])\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.drop_duplicates(subset=['oaid', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create H5PY files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where strings are \"nan\"\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['patent_id'] != \"nan\"]\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['id'] != \"nan\"]\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['oaid'] != \"nan\"]\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['id'] != \"nan\"]\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['id'] != \"nan\"]\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['patent_id'] != \"nan\"]\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['id'] != \"nan\"]\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['oaid'] != \"nan\"]\n",
    "\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['patent_id'] != \"nan\"]\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['citation_patent_id'] != \"nan\"]\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['work_id'] != \"nan\"]\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['referenced_work_id'] != \"nan\"]\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['patent_id'] != \"nan\"]\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['oaid'] != \"nan\"]\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs[df_patent_paper_edge_index_pcs['patent_id'] != \"nan\"]\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs[df_patent_paper_edge_index_pcs['oaid'] != \"nan\"]\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp[df_patent_paper_edge_index_ppp['patent_id'] != \"nan\"]\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp[df_patent_paper_edge_index_ppp['oaid'] != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with nan values\n",
    "df_author_patent_edge_index = df_author_patent_edge_index.dropna(subset=['patent_id', 'id'])\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.dropna(subset=['oaid', 'id'])\n",
    "df_patent_author_edge_index = df_patent_author_edge_index.dropna(subset=['patent_id', 'id'])\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.dropna(subset=['oaid', 'id'])\n",
    "\n",
    "df_patent_edge_index = df_patent_edge_index.dropna(subset=['patent_id', 'citation_patent_id'])\n",
    "df_paper_edge_index = df_paper_edge_index.dropna(subset=['work_id', 'referenced_work_id'])\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index.dropna(subset=['patent_id', 'oaid'])\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs.dropna(subset=['patent_id', 'oaid'])\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp.dropna(subset=['patent_id', 'oaid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patent_edge_index = df_patent_edge_index.astype(int)\n",
    "df_paper_edge_index = df_paper_edge_index.astype(int)\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index.astype(int)\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs.astype(int)\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp.astype(int)\n",
    "df_author_patent_edge_index = df_author_patent_edge_index.astype(int)\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.astype(int)\n",
    "df_patent_author_edge_index = df_patent_author_edge_index.astype(int)\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.astype(int)\n",
    "\n",
    "df_authors['id'] = df_authors['id'].astype(int)\n",
    "df_patstat_text['patentid'] = df_patstat_text['patentid'].astype(int)\n",
    "df_rel_postgres['oaid'] = df_rel_postgres['oaid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['id'] < len(df_authors) - 1]\n",
    "df_author_patent_edge_index = df_author_patent_edge_index[df_author_patent_edge_index['patent_id'] < len(df_patstat_text) - 1]\n",
    "\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['id'] < len(df_authors) - 1]\n",
    "df_patent_author_edge_index = df_patent_author_edge_index[df_patent_author_edge_index['patent_id'] < len(df_patstat_text) - 1]\n",
    "\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['id'] < len(df_authors) - 1]\n",
    "df_author_paper_edge_index = df_author_paper_edge_index[df_author_paper_edge_index['oaid'] < len(df_rel_postgres) - 1]\n",
    "\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['id'] < len(df_authors) - 1]\n",
    "df_paper_author_edge_index = df_paper_author_edge_index[df_paper_author_edge_index['oaid'] < len(df_rel_postgres) - 1]\n",
    "\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['patent_id'] < len(df_patstat_text) - 1]\n",
    "df_patent_edge_index = df_patent_edge_index[df_patent_edge_index['citation_patent_id'] < len(df_patstat_text) - 1]\n",
    "\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['work_id'] < len(df_rel_postgres) - 1]\n",
    "df_paper_edge_index = df_paper_edge_index[df_paper_edge_index['referenced_work_id'] < len(df_rel_postgres) - 1]\n",
    "\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['patent_id'] < len(df_patstat_text) - 1]\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index[df_patent_paper_edge_index['oaid'] < len(df_rel_postgres) - 1]\n",
    "\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs[df_patent_paper_edge_index_pcs['patent_id'] < len(df_patstat_text) - 1]\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs[df_patent_paper_edge_index_pcs['oaid'] < len(df_rel_postgres) - 1]\n",
    "\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp[df_patent_paper_edge_index_ppp['patent_id'] < len(df_patstat_text) - 1]\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp[df_patent_paper_edge_index_ppp['oaid'] < len(df_rel_postgres) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_postgres['oaid'] = df_rel_postgres['oaid'].astype(int)\n",
    "df_rel_info['oaid'] = df_rel_info['oaid'].astype(int)\n",
    "df_rel_postgres = pd.merge(df_rel_postgres, df_rel_info[['oaid', 'patent_paper_pair']], on='oaid', how='inner')\n",
    "df_rel_postgres = df_rel_postgres.dropna(subset=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rel_postgres), len(df_patstat_text), len(df_authors), len(df_patent_edge_index), len(df_paper_edge_index), len(df_patent_paper_edge_index_pcs), len(df_patent_paper_edge_index_ppp), len(df_author_patent_edge_index), len(df_author_paper_edge_index), len(df_patent_author_edge_index), len(df_paper_author_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(str_repr):\n",
    "    return np.fromstring(str_repr.strip('[]'), sep=',')\n",
    "\n",
    "# df_patstat_text['embedding'] = df_patstat_text['embedding'].apply(string_to_array)\n",
    "# df_rel_postgres['embedding'] = df_rel_postgres['embedding'].apply(string_to_array)\n",
    "# df_authors['embedding'] = df_authors['embedding'].apply(string_to_array)\n",
    "\n",
    "# Delete all NaN values from edge indices\n",
    "df_patent_edge_index = df_patent_edge_index.dropna()\n",
    "df_paper_edge_index = df_paper_edge_index.dropna()\n",
    "# df_patent_paper_edge_index = df_patent_paper_edge_index.dropna()\n",
    "df_patent_paper_edge_index_pcs = df_patent_paper_edge_index_pcs.dropna()\n",
    "df_patent_paper_edge_index_ppp = df_patent_paper_edge_index_ppp.dropna()\n",
    "df_author_patent_edge_index = df_author_patent_edge_index.dropna()\n",
    "df_paper_author_edge_index = df_paper_author_edge_index.dropna()\n",
    "df_author_paper_edge_index = df_author_paper_edge_index.dropna()\n",
    "\n",
    "# Open an HDF5 file\n",
    "with h5py.File('/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK PPP/raw/torch_tek_dataset_distilbert_emergence.h5', 'w') as f:\n",
    "    # Save node data\n",
    "    f.create_dataset('g_patent/x', data=np.stack(df_patstat_text[\"embedding\"].values))\n",
    "    f.create_dataset('g_paper/x', data=np.stack(df_rel_postgres[\"embedding\"].values))\n",
    "    # f.create_dataset('g_paper/y', data=np.stack(df_rel_postgres[\"patent_paper_pair\"].values))\n",
    "    f.create_dataset('g_author/x', data=np.stack(df_authors[\"embedding\"].values))\n",
    "    \n",
    "    # Save edge indices\n",
    "    f.create_dataset('patent_edge_index', data=df_patent_edge_index.values, dtype=np.int64)\n",
    "    f.create_dataset('paper_edge_index', data=df_paper_edge_index.values, dtype=np.int64)\n",
    "    f.create_dataset('patent_paper_edge_index', data=df_patent_paper_edge_index_pcs.values, dtype=np.int64)\n",
    "    f.create_dataset('patent_paper_pair_edge_index', data=df_patent_paper_edge_index_ppp.values, dtype=np.int64)\n",
    "    f.create_dataset('author_patent_edge_index', data=df_author_patent_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('patent_author_edge_index', data=df_patent_author_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('author_paper_edge_index', data=df_author_paper_edge_index, dtype=np.int64)\n",
    "    f.create_dataset('paper_author_edge_index', data=df_paper_author_edge_index, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Heterogeneous Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPPHeteroDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PPPHeteroDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        # processed_path = osp.join(self.processed_dir, self.processed_file_names)\n",
    "        # if osp.exists(processed_path):\n",
    "        #     self.data = torch.load(processed_path)\n",
    "        # else:\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return '/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK PPP/raw/'\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return '/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK PPP/processed/'\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\n",
    "            'torch_tek_dataset_distilbert_emergence.h5'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'gnn_tek_data_distilbert_emergence.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Initialize HeteroData object\n",
    "        data = HeteroData()\n",
    "    \n",
    "        # Open an HDF5 file\n",
    "        with h5py.File(osp.join(self.raw_dir, 'torch_tek_dataset_distilbert_emergence.h5'), 'r') as f:\n",
    "            # Load and process node features\n",
    "            data['patent'].x = torch.tensor(f['g_patent/x'][:], dtype=torch.float)\n",
    "            data['paper'].x = torch.tensor(f['g_paper/x'][:], dtype=torch.float)\n",
    "            # data['paper'].y = torch.tensor(f['g_paper/y'][:], dtype=torch.long)\n",
    "\n",
    "            data['author'].x = torch.tensor(f['g_author/x'][:], dtype=torch.float)\n",
    "            \n",
    "            # Load and process edge indices\n",
    "            data['patent', 'cites', 'patent'].edge_index = torch.tensor(f['patent_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['paper', 'cites', 'paper'].edge_index = torch.tensor(f['paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['patent', 'cites', 'paper'].edge_index = torch.tensor(f['patent_paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "\n",
    "            data['author', 'author_of_patent', 'patent'].edge_index = torch.tensor(f['author_patent_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['author', 'author_of_paper', 'paper'].edge_index = torch.tensor(f['author_paper_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['patent', 'has_author_patent', 'author'].edge_index = torch.tensor(f['patent_author_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "            data['paper', 'has_author_paper', 'author'].edge_index = torch.tensor(f['paper_author_edge_index'][:], dtype=torch.long).t().contiguous()\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        # Create train_mask, val_mask, and test_mask\n",
    "        data['paper'].train_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        # data['paper'].val_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        data['paper'].test_mask = torch.zeros(data['paper'].num_nodes, dtype=torch.bool)\n",
    "        data['paper'].train_mask[:int(0.8*data['paper'].num_nodes)] = 1\n",
    "        # data['paper'].val_mask[int(0.8*data['paper'].num_nodes):int(0.9*data['paper'].num_nodes)] = 1]\n",
    "        data['paper'].test_mask[int(0.8*data['paper'].num_nodes):] = 1\n",
    "\n",
    "        # Diagnostic print statements\n",
    "        print(\"Data keys after processing:\", data.keys())\n",
    "        print(\"Node types and their feature shapes:\")\n",
    "        for node_type, node_data in data.node_items():\n",
    "            print(f\"Node type: {node_type}\")\n",
    "            for key, item in node_data.items():\n",
    "                if key == 'x' or key == 'y':\n",
    "                    print(f\"Features ({key}) shape:\", item.size())\n",
    "\n",
    "        print(\"Edge types and their index shapes:\")\n",
    "        for edge_type, edge_data in data.edge_items():\n",
    "            print(f\"Edge type: {edge_type}\")\n",
    "            if 'edge_index' in edge_data:\n",
    "                print(\"Edge index shape:\", edge_data['edge_index'].size())\n",
    "            else:\n",
    "                print(f\"{edge_type} has no edge index.\")\n",
    "        \n",
    "\n",
    "        self.data = data  # Save the processed data to self.data\n",
    "        torch.save(data, osp.join(self.processed_dir, self.processed_file_names))\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys after processing: ['train_mask', 'test_mask', 'edge_index', 'x']\n",
      "Node types and their feature shapes:\n",
      "Node type: patent\n",
      "Features (x) shape: torch.Size([143164, 768])\n",
      "Node type: paper\n",
      "Features (x) shape: torch.Size([120996, 768])\n",
      "Node type: author\n",
      "Features (x) shape: torch.Size([890200, 768])\n",
      "Edge types and their index shapes:\n",
      "Edge type: ('patent', 'cites', 'patent')\n",
      "Edge index shape: torch.Size([2, 55069])\n",
      "Edge type: ('paper', 'cites', 'paper')\n",
      "Edge index shape: torch.Size([2, 278168])\n",
      "Edge type: ('patent', 'cites', 'paper')\n",
      "Edge index shape: torch.Size([2, 119357])\n",
      "Edge type: ('author', 'author_of_patent', 'patent')\n",
      "Edge index shape: torch.Size([2, 618115])\n",
      "Edge type: ('author', 'author_of_paper', 'paper')\n",
      "Edge index shape: torch.Size([2, 252207])\n",
      "Edge type: ('patent', 'has_author_patent', 'author')\n",
      "Edge index shape: torch.Size([2, 618115])\n",
      "Edge type: ('paper', 'has_author_paper', 'author')\n",
      "Edge index shape: torch.Size([2, 252207])\n"
     ]
    }
   ],
   "source": [
    "ppp_dataset = PPPHeteroDataset(root='/mnt/hdd01/patentsview/Graph Neural Network for EDV-TEK PPP/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_dataset_0 = ppp_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullMessagePassing(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super(FullMessagePassing, self).__init__(aggr='mean', flow='source_to_target', node_dim=0) # Aggregation method: \"mean\", \"add\", \"max\", \"min\"\n",
    "\n",
    "    def forward(self, data):\n",
    "        # The `data` is the full HeteroData object\n",
    "        # Iterate over all types of edges defined in the data and perform message passing for each type\n",
    "        for edge_type in data.edge_types:\n",
    "            src, rel, dst = edge_type\n",
    "            if (src, rel, dst) in data.edge_index_dict:\n",
    "                try:\n",
    "                    edge_index = data[src, rel, dst].edge_index\n",
    "                    # data[dst].x = self.propagate(edge_index, x=data[src].x, size=None)\n",
    "                    data[dst].x = self.propagate(edge_index, x=data[src].x)\n",
    "                    # result = data[dst].x\n",
    "                    # data[dst].x = result / result.norm(dim=1, keepdim=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing edge type {src, rel, dst}: {str(e)}\")\n",
    "                    continue\n",
    "        return data\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullMessagePassing().to(device)\n",
    "ppp_dataset_0 = ppp_dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0122843ea2ef461280d6457a4862d191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, 10)):\n",
    "    ppp_dataset_0 = model(ppp_dataset_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0408,  0.4221,  0.4951,  ..., -0.3793, -0.3343,  0.6341],\n",
       "        [ 0.1250,  0.4755,  0.5594,  ..., -0.4141, -0.2955,  0.5524],\n",
       "        [ 0.1467,  0.5069,  0.5324,  ..., -0.3620, -0.3164,  0.5036],\n",
       "        [ 0.0889,  0.4809,  0.4878,  ..., -0.3810, -0.2824,  0.4345],\n",
       "        [ 0.1171,  0.4442,  0.5310,  ..., -0.4051, -0.2892,  0.4538]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 5 entries of node 'patent' in ppp_dataset_0\n",
    "ppp_dataset_0['patent'].x[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGCN(MessagePassing):\n",
    "    def __init__(self, hidden_channels, num_node_features_dict, num_classes):\n",
    "        super(HeteroGCN, self).__init__(aggr='mean')\n",
    "        torch.manual_seed(42) # For reproducible results\n",
    "        \n",
    "        self.conv1 = HeteroConv({\n",
    "            ('patent', 'cites', 'patent'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'cites', 'paper'): SAGEConv(num_node_features_dict['paper'], hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'cites', 'paper'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_patent', 'patent'): SAGEConv(num_node_features_dict['author'], hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_paper', 'paper'): SAGEConv(num_node_features_dict['author'], hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'has_author_patent', 'author'): SAGEConv(num_node_features_dict['patent'], hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'has_author_paper', 'author'): SAGEConv(num_node_features_dict['paper'], hidden_channels, add_self_loops=True)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('patent', 'cites', 'patent'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'cites', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'cites', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_patent', 'patent'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('author', 'author_of_paper', 'paper'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('patent', 'has_author_patent', 'author'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True),\n",
    "            ('paper', 'has_author_paper', 'author'): SAGEConv(hidden_channels, hidden_channels, add_self_loops=True)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict, edge_index_dict = data.x_dict, data.edge_index_dict\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        x_dict = {key: x.mean(dim=0, keepdim=True) for key, x in x_dict.items()} # Global pooling - TEST THIS!!\n",
    "\n",
    "        edge_data = torch.cat([x_dict['patent'], x_dict['paper']], dim=1) # Example for 'patent', 'paper' edge\n",
    "        return self.predictor(edge_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features_dict = {'patent': 768, 'paper': 768, 'author': 768}\n",
    "model = HeteroGCN(hidden_channels=64, num_node_features_dict=num_node_features_dict, num_classes=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = RandomLinkSplit(num_val=0, num_test=0.2, is_undirected=False, edge_types=[('patent', 'cites', 'paper')], add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(ppp_dataset_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 17362,  91313,  73932,  ..., 109586,  51197, 132319],\n",
       "        [ 95899,  13539, 107852,  ...,  31284,  18699,   7637]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['patent', 'cites', 'paper'].edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbor Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "# edge_label_index = ['patent', 'cites', 'paper']\n",
    "# train_loader = LinkNeighborLoader(train_data, batch_size=32, num_neighbors=10, edge_label_index=edge_label_index)\n",
    "# test_loader = LinkNeighborLoader(test_data, batch_size=32, num_neighbors=10, edge_label_index=edge_label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch)  \n",
    "        loss = criterion(out.squeeze(), batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "            pred = (out.squeeze() > 0.5).int()  # Convert probabilities to binary predictions\n",
    "            correct += int((pred == batch.y).sum())\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "    test_acc = correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Explicitly get edge labels and edge indices for your specific edge type\n",
    "    edge_label = data['patent', 'cites', 'paper'].edge_label\n",
    "    edge_index = data['patent', 'cites', 'paper'].edge_index\n",
    "\n",
    "    out = model(data)  \n",
    "    # loss = criterion(out, edge_label.float())\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # total_loss += loss.item()\n",
    "\n",
    "    # return total_loss\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        edge_label = data['patent', 'cites', 'paper'].edge_label\n",
    "        edge_index = data['patent', 'cites', 'paper'].edge_label_index\n",
    "\n",
    "        out = model(data)\n",
    "        pred = (out.squeeze() > 0.5).int()\n",
    "\n",
    "        correct += int((pred == edge_label).sum())  # Count correct predictions\n",
    "        total += edge_label.size(0)  # Total number of edge labels to evaluate\n",
    "\n",
    "    test_acc = correct / total  # Test accuracy\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(train_data)\n",
    "    test_acc = test(test_data)\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
