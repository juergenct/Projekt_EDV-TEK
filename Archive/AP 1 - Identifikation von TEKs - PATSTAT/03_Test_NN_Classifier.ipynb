{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set script parameters\n",
    "SEED = 42 # Set for reproducibility\n",
    "MAX_SEQ_LENGTH = 512  # Maximum sequence length\n",
    "BATCH_SIZE = 8\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# Set random seed for reproducibility on GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Pytorch NN Classifier Script, built using:**\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "- https://medium.com/@spandey8312/text-classification-using-custom-data-and-pytorch-d88ba1087045\n",
    "- https://www.deeplearningwizard.com/deep_learning/intro/\n",
    "\n",
    "**Ideas for improvement:**\n",
    "- Use a different tokenizer\n",
    "- Pytorch (use a different optimizer, scheduler, loss function, learning rate, epochs, batch size, etc.)\n",
    "- Models (adjust model parameter, layer, activation function, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleantech = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/df_patstat_cleantech_granted_abstract_metadata.json')\n",
    "df_cleantech['label'] = 1\n",
    "df_non_cleantech = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/df_patstat_non_cleantech_granted_abstract_metadata.json')\n",
    "df_non_cleantech['label'] = 0\n",
    "df_cleantech = df_cleantech[df_cleantech['appln_abstract'] != '']\n",
    "df_non_cleantech = df_non_cleantech[df_non_cleantech['appln_abstract'] != '']\n",
    "df_cleantech.dropna(inplace=True)\n",
    "df_non_cleantech.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df_cleantech, df_non_cleantech], ignore_index=True)\n",
    "df = pd.concat([df_cleantech.sample(50000, random_state=42), df_non_cleantech.sample(50000, random_state=42)], ignore_index=True)\n",
    "df = df[['appln_id', 'appln_abstract', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:26<00:00, 3746.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df['appln_abstract'] = df['appln_abstract'].astype(str)\n",
    "df['appln_abstract'] = df['appln_abstract'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepraration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:16<00:00, 6100.77it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "df['appln_abstract_tokens'] = df['appln_abstract'].progress_apply(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 1280621.40it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(df['appln_abstract_tokens'].progress_apply(lambda x: x), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collate data into batches for FeedForward Neural Network and Convolutional Neural Network\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collate data into batches for Recurrent Neural Network and Long Short-Term Memory Neural Network\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_text, _label) in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        \n",
    "        # Only add samples where the length is greater than 0\n",
    "        if len(processed_text) > 0:\n",
    "            label_list.append(_label)\n",
    "            text_list.append(processed_text)\n",
    "            lengths.append(len(processed_text))\n",
    "    \n",
    "    # Proceed only if there are samples with non-zero length\n",
    "    if lengths:\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
    "        lengths = torch.tensor(lengths)\n",
    "        return label_list.to(device), text_list.to(device), lengths\n",
    "    else:\n",
    "        # Return None or appropriate default values if all samples had length 0\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'appln_abstract': 'text'})\n",
    "df_torch = df[['text', 'label']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(self.df.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = train_test_split(df_torch, test_size=0.1, random_state=42)\n",
    "train_iter, val_iter = train_test_split(train_iter, test_size=0.1, random_state=42)\n",
    "train_iter = TextClassificationDataset(train_iter.reset_index(drop=True))\n",
    "test_iter = TextClassificationDataset(test_iter.reset_index(drop=True))\n",
    "val_iter = TextClassificationDataset(val_iter.reset_index(drop=True))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "val_dataset = to_map_style_dataset(val_iter)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(FNN, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc1 = nn.Linear(embed_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        return self.fc2(x)\n",
    "# Might also introduce other non-linearities such as Tanh or Sigmoid\n",
    "# Could introduce Dropout layers to prevent overfitting\n",
    "# Could introduce more layers or increase the number of neurons in each layer (embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        final_feature_map = output[torch.arange(output.size(0)), lengths - 1]\n",
    "\n",
    "        return self.fc(final_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        final_feature_map = output[torch.arange(output.size(0)), lengths - 1]\n",
    "\n",
    "        return self.fc(final_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_class):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# model = FNN(len(vocab), 256, 1)\n",
    "# model = CNN(len(vocab), 256, 100, [3, 4, 5], 1) # Filter sizes correspond to trigrams, 4-grams and 5-grams, Number of filters corresponds to number of patterns per each n-gram\n",
    "# model = RNN(len(vocab), 256, 128, 1)\n",
    "model = LSTM(len(vocab), 256, 128, 1)\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 0.1\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop for FeedForward Neural Network and Convolutional Neural Network\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 1000\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label, text = label.to(device), text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_logits = model(text) \n",
    "        loss = criterion(predicted_logits.squeeze(1), label.float()) # For FNN\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        predicted_labels = (torch.sigmoid(predicted_logits) > 0.5).long()\n",
    "        total_acc += (predicted_labels.squeeze(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('Epoch: {:03d} | Batch: {:03d}/{:03d} | Loss: {:03f} | Accuracy: {:.3f}'.format(epoch, idx, len(dataloader), loss.item(), total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop for Recurrent Neural Network and Long Short-Term Memory Neural Network\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 1000\n",
    "    for idx, (label, text, lengths) in enumerate(dataloader):\n",
    "        label, text = label.to(device), text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_logits = model(text, lengths)\n",
    "        loss = criterion(predicted_logits.squeeze(1), label.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        predicted_labels = (torch.sigmoid(predicted_logits) > 0.5).long()\n",
    "        total_acc += (predicted_labels.squeeze(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('Epoch: {:03d} | Batch: {:03d}/{:03d} | Loss: {:03f} | Accuracy: {:.3f}'.format(epoch, idx, len(dataloader), loss.item(), total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop for FeedForward Neural Network and Convolutional Neural Network\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            predicted_logits = model(text)\n",
    "            loss = criterion(predicted_logits.squeeze(1), label.float()) # For FNN\n",
    "            predicted_labels = (torch.sigmoid(predicted_logits) > 0.5).long()\n",
    "            total_acc += (predicted_labels.squeeze(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    \n",
    "        print('End of epoch: {:03d} | Loss: {:03f} | Accuracy: {:.3f}'.format(epoch, loss.item(), total_acc/total_count))\n",
    "        return loss.item(), total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop for Recurrent Neural Network and Long Short-Term Memory Neural Network\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, lengths) in enumerate(dataloader):\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            predicted_logits = model(text, lengths)\n",
    "            loss = criterion(predicted_logits.squeeze(1), label.float())\n",
    "            predicted_labels = (torch.sigmoid(predicted_logits) > 0.5).long()\n",
    "            total_acc += (predicted_labels.squeeze(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    \n",
    "        print('End of epoch: {:03d} | Loss: {:03f} | Accuracy: {:.3f}'.format(epoch, loss.item(), total_acc/total_count))\n",
    "        return loss.item(), total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Batch: 1000/10125 | Loss: 0.760616 | Accuracy: 0.506\n",
      "Epoch: 001 | Batch: 2000/10125 | Loss: 1.405729 | Accuracy: 0.507\n",
      "Epoch: 001 | Batch: 3000/10125 | Loss: 0.858329 | Accuracy: 0.505\n",
      "Epoch: 001 | Batch: 4000/10125 | Loss: 0.747641 | Accuracy: 0.509\n",
      "Epoch: 001 | Batch: 5000/10125 | Loss: 0.641245 | Accuracy: 0.510\n",
      "Epoch: 001 | Batch: 6000/10125 | Loss: 0.931087 | Accuracy: 0.520\n",
      "Epoch: 001 | Batch: 7000/10125 | Loss: 1.183208 | Accuracy: 0.515\n",
      "Epoch: 001 | Batch: 8000/10125 | Loss: 1.720895 | Accuracy: 0.525\n",
      "Epoch: 001 | Batch: 9000/10125 | Loss: 0.760823 | Accuracy: 0.512\n",
      "Epoch: 001 | Batch: 10000/10125 | Loss: 0.912768 | Accuracy: 0.501\n",
      "------------------------------------------------------------\n",
      "End of epoch: 001 | Loss: 1.254563 | Accuracy: 0.506\n",
      "------------------------------------------------------------\n",
      "Epoch: 002 | Batch: 1000/10125 | Loss: 0.639118 | Accuracy: 0.515\n",
      "Epoch: 002 | Batch: 2000/10125 | Loss: 0.743480 | Accuracy: 0.528\n",
      "Epoch: 002 | Batch: 3000/10125 | Loss: 0.772383 | Accuracy: 0.521\n",
      "Epoch: 002 | Batch: 4000/10125 | Loss: 0.780544 | Accuracy: 0.523\n",
      "Epoch: 002 | Batch: 5000/10125 | Loss: 0.746557 | Accuracy: 0.513\n",
      "Epoch: 002 | Batch: 6000/10125 | Loss: 0.661713 | Accuracy: 0.529\n",
      "Epoch: 002 | Batch: 7000/10125 | Loss: 0.644374 | Accuracy: 0.527\n",
      "Epoch: 002 | Batch: 8000/10125 | Loss: 0.656924 | Accuracy: 0.528\n",
      "Epoch: 002 | Batch: 9000/10125 | Loss: 0.641806 | Accuracy: 0.516\n",
      "Epoch: 002 | Batch: 10000/10125 | Loss: 0.520718 | Accuracy: 0.525\n",
      "------------------------------------------------------------\n",
      "End of epoch: 002 | Loss: 0.733335 | Accuracy: 0.525\n",
      "------------------------------------------------------------\n",
      "Epoch: 003 | Batch: 1000/10125 | Loss: 0.698925 | Accuracy: 0.543\n",
      "Epoch: 003 | Batch: 2000/10125 | Loss: 0.768740 | Accuracy: 0.538\n",
      "Epoch: 003 | Batch: 3000/10125 | Loss: 0.681728 | Accuracy: 0.551\n",
      "Epoch: 003 | Batch: 4000/10125 | Loss: 0.686757 | Accuracy: 0.546\n",
      "Epoch: 003 | Batch: 5000/10125 | Loss: 0.761244 | Accuracy: 0.542\n",
      "Epoch: 003 | Batch: 6000/10125 | Loss: 0.746435 | Accuracy: 0.543\n",
      "Epoch: 003 | Batch: 7000/10125 | Loss: 0.752058 | Accuracy: 0.530\n",
      "Epoch: 003 | Batch: 8000/10125 | Loss: 0.685139 | Accuracy: 0.550\n",
      "Epoch: 003 | Batch: 9000/10125 | Loss: 0.655182 | Accuracy: 0.554\n",
      "Epoch: 003 | Batch: 10000/10125 | Loss: 0.673661 | Accuracy: 0.550\n",
      "------------------------------------------------------------\n",
      "End of epoch: 003 | Loss: 0.642367 | Accuracy: 0.531\n",
      "------------------------------------------------------------\n",
      "Epoch: 004 | Batch: 1000/10125 | Loss: 0.659376 | Accuracy: 0.564\n",
      "Epoch: 004 | Batch: 2000/10125 | Loss: 0.702114 | Accuracy: 0.557\n",
      "Epoch: 004 | Batch: 3000/10125 | Loss: 0.730215 | Accuracy: 0.559\n",
      "Epoch: 004 | Batch: 4000/10125 | Loss: 0.649355 | Accuracy: 0.572\n",
      "Epoch: 004 | Batch: 5000/10125 | Loss: 0.666813 | Accuracy: 0.564\n",
      "Epoch: 004 | Batch: 6000/10125 | Loss: 0.791646 | Accuracy: 0.565\n",
      "Epoch: 004 | Batch: 7000/10125 | Loss: 0.623361 | Accuracy: 0.561\n",
      "Epoch: 004 | Batch: 8000/10125 | Loss: 0.627084 | Accuracy: 0.567\n",
      "Epoch: 004 | Batch: 9000/10125 | Loss: 0.711540 | Accuracy: 0.567\n",
      "Epoch: 004 | Batch: 10000/10125 | Loss: 0.584158 | Accuracy: 0.562\n",
      "------------------------------------------------------------\n",
      "End of epoch: 004 | Loss: 0.775949 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 005 | Batch: 1000/10125 | Loss: 0.748800 | Accuracy: 0.569\n",
      "Epoch: 005 | Batch: 2000/10125 | Loss: 0.707415 | Accuracy: 0.569\n",
      "Epoch: 005 | Batch: 3000/10125 | Loss: 0.631383 | Accuracy: 0.580\n",
      "Epoch: 005 | Batch: 4000/10125 | Loss: 0.794233 | Accuracy: 0.567\n",
      "Epoch: 005 | Batch: 5000/10125 | Loss: 0.693309 | Accuracy: 0.580\n",
      "Epoch: 005 | Batch: 6000/10125 | Loss: 0.756791 | Accuracy: 0.571\n",
      "Epoch: 005 | Batch: 7000/10125 | Loss: 0.692802 | Accuracy: 0.568\n",
      "Epoch: 005 | Batch: 8000/10125 | Loss: 0.726981 | Accuracy: 0.563\n",
      "Epoch: 005 | Batch: 9000/10125 | Loss: 0.594733 | Accuracy: 0.566\n",
      "Epoch: 005 | Batch: 10000/10125 | Loss: 0.578640 | Accuracy: 0.579\n",
      "------------------------------------------------------------\n",
      "End of epoch: 005 | Loss: 0.745593 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 006 | Batch: 1000/10125 | Loss: 0.725990 | Accuracy: 0.575\n",
      "Epoch: 006 | Batch: 2000/10125 | Loss: 0.627477 | Accuracy: 0.568\n",
      "Epoch: 006 | Batch: 3000/10125 | Loss: 0.689358 | Accuracy: 0.566\n",
      "Epoch: 006 | Batch: 4000/10125 | Loss: 0.813907 | Accuracy: 0.567\n",
      "Epoch: 006 | Batch: 5000/10125 | Loss: 0.757005 | Accuracy: 0.566\n",
      "Epoch: 006 | Batch: 6000/10125 | Loss: 0.634685 | Accuracy: 0.573\n",
      "Epoch: 006 | Batch: 7000/10125 | Loss: 0.721370 | Accuracy: 0.574\n",
      "Epoch: 006 | Batch: 8000/10125 | Loss: 0.628009 | Accuracy: 0.573\n",
      "Epoch: 006 | Batch: 9000/10125 | Loss: 0.741506 | Accuracy: 0.576\n",
      "Epoch: 006 | Batch: 10000/10125 | Loss: 0.691170 | Accuracy: 0.573\n",
      "------------------------------------------------------------\n",
      "End of epoch: 006 | Loss: 0.749088 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 007 | Batch: 1000/10125 | Loss: 0.782640 | Accuracy: 0.579\n",
      "Epoch: 007 | Batch: 2000/10125 | Loss: 0.583924 | Accuracy: 0.576\n",
      "Epoch: 007 | Batch: 3000/10125 | Loss: 0.730118 | Accuracy: 0.568\n",
      "Epoch: 007 | Batch: 4000/10125 | Loss: 0.713479 | Accuracy: 0.571\n",
      "Epoch: 007 | Batch: 5000/10125 | Loss: 0.902539 | Accuracy: 0.566\n",
      "Epoch: 007 | Batch: 6000/10125 | Loss: 0.663393 | Accuracy: 0.564\n",
      "Epoch: 007 | Batch: 7000/10125 | Loss: 0.725142 | Accuracy: 0.574\n",
      "Epoch: 007 | Batch: 8000/10125 | Loss: 0.790747 | Accuracy: 0.566\n",
      "Epoch: 007 | Batch: 9000/10125 | Loss: 0.763047 | Accuracy: 0.576\n",
      "Epoch: 007 | Batch: 10000/10125 | Loss: 0.653133 | Accuracy: 0.576\n",
      "------------------------------------------------------------\n",
      "End of epoch: 007 | Loss: 0.709285 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 008 | Batch: 1000/10125 | Loss: 0.556201 | Accuracy: 0.578\n",
      "Epoch: 008 | Batch: 2000/10125 | Loss: 0.611283 | Accuracy: 0.572\n",
      "Epoch: 008 | Batch: 3000/10125 | Loss: 0.646496 | Accuracy: 0.577\n",
      "Epoch: 008 | Batch: 4000/10125 | Loss: 0.594892 | Accuracy: 0.571\n",
      "Epoch: 008 | Batch: 5000/10125 | Loss: 0.619037 | Accuracy: 0.566\n",
      "Epoch: 008 | Batch: 6000/10125 | Loss: 0.612415 | Accuracy: 0.582\n",
      "Epoch: 008 | Batch: 7000/10125 | Loss: 0.646387 | Accuracy: 0.562\n",
      "Epoch: 008 | Batch: 8000/10125 | Loss: 0.685660 | Accuracy: 0.564\n",
      "Epoch: 008 | Batch: 9000/10125 | Loss: 0.618772 | Accuracy: 0.569\n",
      "Epoch: 008 | Batch: 10000/10125 | Loss: 0.699310 | Accuracy: 0.575\n",
      "------------------------------------------------------------\n",
      "End of epoch: 008 | Loss: 0.725598 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 009 | Batch: 1000/10125 | Loss: 0.702866 | Accuracy: 0.565\n",
      "Epoch: 009 | Batch: 2000/10125 | Loss: 0.627241 | Accuracy: 0.577\n",
      "Epoch: 009 | Batch: 3000/10125 | Loss: 0.624663 | Accuracy: 0.565\n",
      "Epoch: 009 | Batch: 4000/10125 | Loss: 0.625933 | Accuracy: 0.577\n",
      "Epoch: 009 | Batch: 5000/10125 | Loss: 0.807017 | Accuracy: 0.578\n",
      "Epoch: 009 | Batch: 6000/10125 | Loss: 0.645631 | Accuracy: 0.577\n",
      "Epoch: 009 | Batch: 7000/10125 | Loss: 0.692287 | Accuracy: 0.576\n",
      "Epoch: 009 | Batch: 8000/10125 | Loss: 0.683672 | Accuracy: 0.557\n",
      "Epoch: 009 | Batch: 9000/10125 | Loss: 0.629833 | Accuracy: 0.568\n",
      "Epoch: 009 | Batch: 10000/10125 | Loss: 0.787235 | Accuracy: 0.574\n",
      "------------------------------------------------------------\n",
      "End of epoch: 009 | Loss: 0.700871 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n",
      "Epoch: 010 | Batch: 1000/10125 | Loss: 0.571454 | Accuracy: 0.572\n",
      "Epoch: 010 | Batch: 2000/10125 | Loss: 0.520234 | Accuracy: 0.567\n",
      "Epoch: 010 | Batch: 3000/10125 | Loss: 0.619250 | Accuracy: 0.576\n",
      "Epoch: 010 | Batch: 4000/10125 | Loss: 0.559259 | Accuracy: 0.574\n",
      "Epoch: 010 | Batch: 5000/10125 | Loss: 0.687317 | Accuracy: 0.575\n",
      "Epoch: 010 | Batch: 6000/10125 | Loss: 0.682953 | Accuracy: 0.566\n",
      "Epoch: 010 | Batch: 7000/10125 | Loss: 0.710016 | Accuracy: 0.579\n",
      "Epoch: 010 | Batch: 8000/10125 | Loss: 0.724151 | Accuracy: 0.570\n",
      "Epoch: 010 | Batch: 9000/10125 | Loss: 0.650059 | Accuracy: 0.570\n",
      "Epoch: 010 | Batch: 10000/10125 | Loss: 0.619017 | Accuracy: 0.569\n",
      "------------------------------------------------------------\n",
      "End of epoch: 010 | Loss: 0.613795 | Accuracy: 0.537\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(train_dataloader)\n",
    "    print(\"-\"*60)\n",
    "    accu_val = evaluate(val_dataloader)\n",
    "    print(\"-\"*60)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
