{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name distilbert/distilbert-base-uncased. Creating a new one with mean pooling.\n",
      "/home/thiesen/.conda/envs/edv_tek/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import h5py\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "model = SentenceTransformer(model_name).to(device)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cleantech and Non-Cleantech Text Data\n",
    "df_cleantech = pd.read_parquet('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_all_cleantech_title_abstract.parquet')\n",
    "df_cleantech = df_cleantech.astype(str)\n",
    "df_cleantech = df_cleantech.dropna(subset=['appln_abstract'])\n",
    "df_cleantech['label'] = 1\n",
    "\n",
    "df_non_cleantech = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_non_cleantech_all_title_abstract.csv')\n",
    "df_non_cleantech = df_non_cleantech.astype(str)\n",
    "df_non_cleantech = df_non_cleantech.dropna(subset=['appln_abstract'])\n",
    "df_non_cleantech['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleantech to non-cleantech citations first to ensure inclusion\n",
    "df_cleantech_non_cleantech_citations = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_cleantech_to_non_cleantech_citations.csv')\n",
    "df_cleantech_non_cleantech_citations = df_cleantech_non_cleantech_citations.astype(str)\n",
    "\n",
    "# Sample cross-citation pairs to include (adjust sample size as needed)\n",
    "sample_size = min(30000, len(df_cleantech_non_cleantech_citations))\n",
    "cross_citation_sample = df_cleantech_non_cleantech_citations.sample(sample_size, random_state=SEED)\n",
    "\n",
    "# Rename columns \n",
    "cross_citation_sample.rename(columns={\"citing_appln_id\": \"pat_appln_id\", \"cited_appln_id\": \"cited_pat_appln_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleantech patents from cross-citations: 27559\n",
      "Number of non-cleantech patents from cross-citations: 28753\n"
     ]
    }
   ],
   "source": [
    "# Get unique patent IDs from both sides of the citations\n",
    "cleantech_patents_to_include = cross_citation_sample['pat_appln_id'].unique().tolist()\n",
    "non_cleantech_patents_to_include = cross_citation_sample['cited_pat_appln_id'].unique().tolist()\n",
    "\n",
    "print(f\"Number of cleantech patents from cross-citations: {len(cleantech_patents_to_include)}\")\n",
    "print(f\"Number of non-cleantech patents from cross-citations: {len(non_cleantech_patents_to_include)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22200 cleantech patents to include\n",
      "Found 23575 non-cleantech patents to include\n"
     ]
    }
   ],
   "source": [
    "# Filter to get patents with cross-citations\n",
    "df_cleantech_must_include = df_cleantech[df_cleantech['appln_id'].isin(cleantech_patents_to_include)]\n",
    "df_non_cleantech_must_include = df_non_cleantech[df_non_cleantech['appln_id'].isin(non_cleantech_patents_to_include)]\n",
    "\n",
    "print(f\"Found {len(df_cleantech_must_include)} cleantech patents to include\")\n",
    "print(f\"Found {len(df_non_cleantech_must_include)} non-cleantech patents to include\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 77800 additional cleantech patents\n",
      "Sampling 76425 additional non-cleantech patents\n"
     ]
    }
   ],
   "source": [
    "# Calculate remaining patents to sample\n",
    "remaining_cleantech = df_cleantech[~df_cleantech['appln_id'].isin(cleantech_patents_to_include)]\n",
    "remaining_non_cleantech = df_non_cleantech[~df_non_cleantech['appln_id'].isin(non_cleantech_patents_to_include)]\n",
    "\n",
    "# Calculate how many more patents to sample to reach 100k each\n",
    "n_cleantech_to_sample = 100000 - len(df_cleantech_must_include)\n",
    "n_non_cleantech_to_sample = 100000 - len(df_non_cleantech_must_include)\n",
    "\n",
    "print(f\"Sampling {n_cleantech_to_sample} additional cleantech patents\")\n",
    "print(f\"Sampling {n_non_cleantech_to_sample} additional non-cleantech patents\")\n",
    "\n",
    "# Sample additional patents\n",
    "df_cleantech_additional = remaining_cleantech.sample(\n",
    "    min(n_cleantech_to_sample, len(remaining_cleantech)), \n",
    "    random_state=SEED\n",
    ")\n",
    "df_non_cleantech_additional = remaining_non_cleantech.sample(\n",
    "    min(n_non_cleantech_to_sample, len(remaining_non_cleantech)), \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the must-include patents with the additional samples\n",
    "df_cleantech_final = pd.concat([df_cleantech_must_include, df_cleantech_additional])\n",
    "df_non_cleantech_final = pd.concat([df_non_cleantech_must_include, df_non_cleantech_additional])\n",
    "\n",
    "# Create the final text dataframe\n",
    "df_text = pd.concat([df_cleantech_final, df_non_cleantech_final], ignore_index=True)\n",
    "df_text = df_text.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:29<00:00, 6685.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Preprocess text data\n",
    "df_text['appln_abstract'] = df_text['appln_abstract'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cleantech and Non-Cleantech Authors Data\n",
    "df_cleantech_authors = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_all_cleantech_patstat_person_id.csv')\n",
    "df_cleantech_authors = df_cleantech_authors.astype(str)\n",
    "df_non_cleantech_authors = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_non_cleantech_all_patstat_person_id.csv')\n",
    "df_non_cleantech_authors = df_non_cleantech_authors.astype(str)\n",
    "df_cleantech_authors = df_cleantech_authors[df_cleantech_authors['appln_id'].isin(df_text['appln_id'])]\n",
    "df_non_cleantech_authors = df_non_cleantech_authors[df_non_cleantech_authors['appln_id'].isin(df_text['appln_id'])]\n",
    "df_authors_edges = pd.concat([df_cleantech_authors, df_non_cleantech_authors], ignore_index=True)\n",
    "df_authors_nodes = pd.DataFrame({'person_id': df_authors_edges['person_id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Non-Cleantech Citations Data as they are very large\n",
    "df_non_cleantech_citations = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_non_cleantech_all_patstat_citations.csv')\n",
    "df_non_cleantech_citations = df_non_cleantech_citations.astype(str)\n",
    "df_non_cleantech_citations = df_non_cleantech_citations[\n",
    "    df_non_cleantech_citations['pat_appln_id'].isin(df_text['appln_id'])\n",
    "]\n",
    "df_non_cleantech_citations = df_non_cleantech_citations[\n",
    "    df_non_cleantech_citations['cited_pat_appln_id'].isin(df_text['appln_id'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cleantech and Non-Cleantech Citation Data\n",
    "df_cleantech_citations = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_all_cleantech_patstat_citations.csv')\n",
    "df_cleantech_citations = df_cleantech_citations.astype(str)\n",
    "\n",
    "# Filter the citations to only include the patents in our sample\n",
    "df_cleantech_citations = df_cleantech_citations[\n",
    "    df_cleantech_citations['pat_appln_id'].isin(df_text['appln_id'])\n",
    "]\n",
    "df_cleantech_citations = df_cleantech_citations[\n",
    "    df_cleantech_citations['cited_pat_appln_id'].isin(df_text['appln_id'])\n",
    "]\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_cleantech_non_cleantech_citations.rename(\n",
    "    columns={\"citing_appln_id\": \"pat_appln_id\", \"cited_appln_id\": \"cited_pat_appln_id\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Filter the cross-citations to only include the patents in our sample\n",
    "df_cleantech_non_cleantech_citations = df_cleantech_non_cleantech_citations[\n",
    "    df_cleantech_non_cleantech_citations['pat_appln_id'].isin(df_text['appln_id'])\n",
    "]\n",
    "df_cleantech_non_cleantech_citations = df_cleantech_non_cleantech_citations[\n",
    "    df_cleantech_non_cleantech_citations['cited_pat_appln_id'].isin(df_text['appln_id'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all citation datasets\n",
    "df_citations = pd.concat([df_cleantech_citations, df_non_cleantech_citations, df_cleantech_non_cleantech_citations], ignore_index=True)\n",
    "df_citations = df_citations[df_citations['pat_appln_id'].isin(df_text['appln_id'])]\n",
    "df_citations = df_citations[df_citations['cited_pat_appln_id'].isin(df_text['appln_id'])]\n",
    "df_citations = df_citations.drop_duplicates(subset=['pat_appln_id', 'cited_pat_appln_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts for pat_appln_id:\n",
      "pat_label\n",
      "1    400123\n",
      "0    132387\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label counts for cited_pat_appln_id:\n",
      "cited_label\n",
      "0    409977\n",
      "1    122533\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of rows where pat_appln_id and cited_pat_appln_id have different labels:\n",
      "288986\n"
     ]
    }
   ],
   "source": [
    "### Label Mapping Check ###\n",
    "# Create a mapping from appln_id to label in df_text\n",
    "label_mapping = df_text.set_index('appln_id')['label']\n",
    "\n",
    "# Map labels to the citations dataframe\n",
    "df_citations['pat_label'] = df_citations['pat_appln_id'].map(label_mapping)\n",
    "df_citations['cited_label'] = df_citations['cited_pat_appln_id'].map(label_mapping)\n",
    "\n",
    "# Count the occurrences in each column (ignoring non-matches, i.e. NaN)\n",
    "pat_counts = df_citations['pat_label'].value_counts(dropna=True)\n",
    "cited_counts = df_citations['cited_label'].value_counts(dropna=True)\n",
    "\n",
    "print(\"Label counts for pat_appln_id:\")\n",
    "print(pat_counts)\n",
    "print(\"\\nLabel counts for cited_pat_appln_id:\")\n",
    "print(cited_counts)\n",
    "\n",
    "# For comparison, consider only rows where both labels are found\n",
    "df_valid = df_citations.dropna(subset=['pat_label', 'cited_label'])\n",
    "different_labels = (df_valid['pat_label'] != df_valid['cited_label']).sum()\n",
    "\n",
    "print(\"\\nNumber of rows where pat_appln_id and cited_pat_appln_id have different labels:\")\n",
    "print(different_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 535234, 756150, 387905, 368245, 532510)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_text), len(df_authors_nodes), len(df_authors_edges), len(df_cleantech_authors), len(df_non_cleantech_authors), len(df_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings for Patents and Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55977c2dd7a747faa2d2315a4ea6ae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embed Text Data\n",
    "embeddings_text = model.encode(\n",
    "    df_text['appln_abstract'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Move tensor to CPU and convert to NumPy array before scaling\n",
    "embeddings_text = scaler.fit_transform(embeddings_text.cpu().numpy())\n",
    "\n",
    "df_text['embeddings'] = embeddings_text.tolist()\n",
    "df_text.to_csv(f'/mnt/hdd02/Projekt_EDV_TEK/gnn_dataset_identification/edv_tek_all_text_embeddings_distilbert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535234/535234 [00:07<00:00, 67555.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Precompute mapping: person_id -> list of appln_ids (from df_authors_edges)\n",
    "person_to_applns = df_authors_edges.groupby('person_id')['appln_id'].agg(list).to_dict()\n",
    "\n",
    "# Precompute mapping: appln_id -> index in df_text\n",
    "appln_to_index = df_text.reset_index().set_index('appln_id')['index'].to_dict()\n",
    "\n",
    "author_features = {}\n",
    "for person_id in tqdm(df_authors_nodes['person_id']):\n",
    "    # Get all application IDs for this author using the precomputed mapping\n",
    "    author_appln_ids = person_to_applns.get(person_id, [])\n",
    "    \n",
    "    # Get indices quickly via dictionary lookup\n",
    "    indices = [appln_to_index[appln] for appln in author_appln_ids if appln in appln_to_index]\n",
    "    \n",
    "    if indices:\n",
    "        # Average the embeddings for these indices\n",
    "        author_emb = np.mean(embeddings_text[indices], axis=0)\n",
    "    else:\n",
    "        # Fallback to random initialization if no patents are found\n",
    "        author_emb = np.random.normal(size=embeddings_text.shape[1])\n",
    "    \n",
    "    author_features[person_id] = author_emb\n",
    "\n",
    "# Add embeddings to the authors DataFrame\n",
    "df_authors_nodes['embeddings'] = df_authors_nodes['person_id'].map(author_features).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edge Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index column\n",
    "df_text['id'] = df_text.index\n",
    "df_authors_nodes['id'] = df_authors_nodes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Edges DataFrames\n",
    "df_authors_edges['author_id'] = df_authors_edges['person_id'].map(df_authors_nodes.set_index('person_id')['id'])\n",
    "df_authors_edges['text_id'] = df_authors_edges['appln_id'].map(df_text.set_index('appln_id')['id'])\n",
    "df_authors_edges = df_authors_edges.dropna(subset=['author_id', 'text_id'])\n",
    "\n",
    "df_citations_edges = df_citations.copy()\n",
    "df_citations_edges['text_id'] = df_citations_edges['pat_appln_id'].map(df_text.set_index('appln_id')['id'])\n",
    "df_citations_edges['cited_text_id'] = df_citations_edges['cited_pat_appln_id'].map(df_text.set_index('appln_id')['id'])\n",
    "df_citations_edges = df_citations_edges.dropna(subset=['text_id', 'cited_text_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values to integers\n",
    "df_authors_edges['author_id'] = df_authors_edges['author_id'].astype(int)\n",
    "df_authors_edges['text_id'] = df_authors_edges['text_id'].astype(int)\n",
    "df_citations_edges['text_id'] = df_citations_edges['text_id'].astype(int)\n",
    "df_citations_edges['cited_text_id'] = df_citations_edges['cited_text_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build HDF5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(str_repr):\n",
    "    return np.fromstring(str_repr.strip('[]'), sep=',')\n",
    "\n",
    "# Open an HDF5 file\n",
    "with h5py.File('/mnt/hdd02/Projekt_EDV_TEK/gnn_dataset_identification/edv_tek_identification_gnn_dataset.h5', 'w') as f:\n",
    "    # Save node data\n",
    "    f.create_dataset('patent_nodes/x', data=np.stack(df_text['embeddings'].values))\n",
    "    f.create_dataset('patent_nodes/y', data=df_text['label'].values.astype(np.int64))\n",
    "    f.create_dataset('author_nodes/x', data=np.stack(df_authors_nodes['embeddings'].values))\n",
    "    \n",
    "    # Save edge indices\n",
    "    f.create_dataset('patent_citations', data=df_citations_edges[['text_id', 'cited_text_id']].values, dtype=np.int64)\n",
    "    f.create_dataset('author_patent_edges', data=df_authors_edges[['author_id', 'text_id']].values, dtype=np.int64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edv_tek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
