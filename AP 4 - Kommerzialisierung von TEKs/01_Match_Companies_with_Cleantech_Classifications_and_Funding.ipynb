{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f00e39",
   "metadata": {},
   "source": [
    "# Load Company Matches CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6428c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_matches = pd.read_csv('/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_results/company_matches_20250712_172703.csv')\n",
    "df_people_matches = pd.read_csv('/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_results/people_matches_20250712_172703.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ab2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_matches = df_company_matches[['cb_uuid', 'cb_name', 'cb_harmonized', 'ps_person_id', 'ps_name', 'ps_harmonized', 'country_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1783597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_matches = df_people_matches[['cb_uuid', 'cb_name', 'cb_harmonized', 'ps_person_id', 'ps_name', 'ps_harmonized']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f95a0",
   "metadata": {},
   "source": [
    "# Match with all Cleantech Patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1982eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patstat_person_id = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_all_cleantech_patstat_person_id.csv')\n",
    "df_patstat_cleantech = pd.read_parquet('/mnt/hdd02/Projekt_EDV_TEK/edv_tek_cleantech_patents_title_abstract_date_cpc_ipc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83babfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5938627, 580172, 8086061, 1870863)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_company_matches), len(df_people_matches), len(df_patstat_person_id), len(df_patstat_cleantech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630d917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cb_uuid</th>\n",
       "      <th>cb_name</th>\n",
       "      <th>cb_harmonized</th>\n",
       "      <th>ps_person_id</th>\n",
       "      <th>ps_name</th>\n",
       "      <th>ps_harmonized</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21e77067-5537-408e-cad7-e5e72bb6ad86</td>\n",
       "      <td>Scribd</td>\n",
       "      <td>SCRIBD</td>\n",
       "      <td>11901439</td>\n",
       "      <td>Scribd, Inc.</td>\n",
       "      <td>SCRIBD</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11352c70-21bb-b546-e9b2-723a4be03075</td>\n",
       "      <td>OpenX</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>11521421</td>\n",
       "      <td>OpenX Technologies, Inc.</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11352c70-21bb-b546-e9b2-723a4be03075</td>\n",
       "      <td>OpenX</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>11521421</td>\n",
       "      <td>OpenX Technologies, Inc.</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11352c70-21bb-b546-e9b2-723a4be03075</td>\n",
       "      <td>OpenX</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>11521421</td>\n",
       "      <td>OpenX Technologies, Inc.</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11352c70-21bb-b546-e9b2-723a4be03075</td>\n",
       "      <td>OpenX</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>11521421</td>\n",
       "      <td>OpenX Technologies, Inc.</td>\n",
       "      <td>OPENX</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                cb_uuid cb_name cb_harmonized  ps_person_id  \\\n",
       "0  21e77067-5537-408e-cad7-e5e72bb6ad86  Scribd        SCRIBD      11901439   \n",
       "1  11352c70-21bb-b546-e9b2-723a4be03075   OpenX         OPENX      11521421   \n",
       "2  11352c70-21bb-b546-e9b2-723a4be03075   OpenX         OPENX      11521421   \n",
       "3  11352c70-21bb-b546-e9b2-723a4be03075   OpenX         OPENX      11521421   \n",
       "4  11352c70-21bb-b546-e9b2-723a4be03075   OpenX         OPENX      11521421   \n",
       "\n",
       "                    ps_name ps_harmonized country_code  \n",
       "0              Scribd, Inc.        SCRIBD          USA  \n",
       "1  OpenX Technologies, Inc.         OPENX          USA  \n",
       "2  OpenX Technologies, Inc.         OPENX          USA  \n",
       "3  OpenX Technologies, Inc.         OPENX          USA  \n",
       "4  OpenX Technologies, Inc.         OPENX          USA  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_company_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca863e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_matches_patstat = pd.merge(df_people_matches, df_patstat_person_id, left_on='ps_person_id', right_on='person_id', how='inner')\n",
    "df_people_matches_patstat.drop_duplicates(subset=['ps_person_id', 'appln_id'], inplace=True)\n",
    "df_people_matches_patstat.to_csv('/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_cb_patents/people_matches_patstat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cfb7c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1: rows 0-50000 (50000 rows, 0.8% complete)\n",
      "  → Found 49471 matches, saved to chunk_001.csv\n",
      "Processing chunk 2: rows 50000-100000 (50000 rows, 1.7% complete)\n",
      "  → Found 49440 matches, saved to chunk_002.csv\n",
      "Processing chunk 3: rows 100000-150000 (50000 rows, 2.5% complete)\n",
      "  → Found 49328 matches, saved to chunk_003.csv\n",
      "Processing chunk 4: rows 150000-200000 (50000 rows, 3.4% complete)\n",
      "  → Found 47273 matches, saved to chunk_004.csv\n",
      "Processing chunk 5: rows 200000-250000 (50000 rows, 4.2% complete)\n",
      "  → Found 49662 matches, saved to chunk_005.csv\n",
      "Processing chunk 6: rows 250000-300000 (50000 rows, 5.1% complete)\n",
      "  → Found 51015 matches, saved to chunk_006.csv\n",
      "Processing chunk 7: rows 300000-350000 (50000 rows, 5.9% complete)\n",
      "  → Found 52395 matches, saved to chunk_007.csv\n",
      "Processing chunk 8: rows 350000-400000 (50000 rows, 6.7% complete)\n",
      "  → Found 43879 matches, saved to chunk_008.csv\n",
      "Processing chunk 9: rows 400000-450000 (50000 rows, 7.6% complete)\n",
      "  → Found 46565 matches, saved to chunk_009.csv\n",
      "Processing chunk 10: rows 450000-500000 (50000 rows, 8.4% complete)\n",
      "  → Found 50103 matches, saved to chunk_010.csv\n",
      "Processing chunk 11: rows 500000-550000 (50000 rows, 9.3% complete)\n",
      "  → Found 44937 matches, saved to chunk_011.csv\n",
      "Processing chunk 12: rows 550000-600000 (50000 rows, 10.1% complete)\n",
      "  → Found 47922 matches, saved to chunk_012.csv\n",
      "Processing chunk 13: rows 600000-650000 (50000 rows, 10.9% complete)\n",
      "  → Found 44239 matches, saved to chunk_013.csv\n",
      "Processing chunk 14: rows 650000-700000 (50000 rows, 11.8% complete)\n",
      "  → Found 37052 matches, saved to chunk_014.csv\n",
      "Processing chunk 15: rows 700000-750000 (50000 rows, 12.6% complete)\n",
      "  → Found 42090 matches, saved to chunk_015.csv\n",
      "Processing chunk 16: rows 750000-800000 (50000 rows, 13.5% complete)\n",
      "  → Found 48253 matches, saved to chunk_016.csv\n",
      "Processing chunk 17: rows 800000-850000 (50000 rows, 14.3% complete)\n",
      "  → Found 44828 matches, saved to chunk_017.csv\n",
      "Processing chunk 18: rows 850000-900000 (50000 rows, 15.2% complete)\n",
      "  → Found 42308 matches, saved to chunk_018.csv\n",
      "Processing chunk 19: rows 900000-950000 (50000 rows, 16.0% complete)\n",
      "  → Found 53446 matches, saved to chunk_019.csv\n",
      "Processing chunk 20: rows 950000-1000000 (50000 rows, 16.8% complete)\n",
      "  → Found 49604 matches, saved to chunk_020.csv\n",
      "Processing chunk 21: rows 1000000-1050000 (50000 rows, 17.7% complete)\n",
      "  → Found 44200 matches, saved to chunk_021.csv\n",
      "Processing chunk 22: rows 1050000-1100000 (50000 rows, 18.5% complete)\n",
      "  → Found 19507 matches, saved to chunk_022.csv\n",
      "Processing chunk 23: rows 1100000-1150000 (50000 rows, 19.4% complete)\n",
      "  → Found 44695 matches, saved to chunk_023.csv\n",
      "Processing chunk 24: rows 1150000-1200000 (50000 rows, 20.2% complete)\n",
      "  → Found 28918 matches, saved to chunk_024.csv\n",
      "Processing chunk 25: rows 1200000-1250000 (50000 rows, 21.0% complete)\n",
      "  → Found 36296 matches, saved to chunk_025.csv\n",
      "Processing chunk 26: rows 1250000-1300000 (50000 rows, 21.9% complete)\n",
      "  → Found 47121 matches, saved to chunk_026.csv\n",
      "Processing chunk 27: rows 1300000-1350000 (50000 rows, 22.7% complete)\n",
      "  → Found 41886 matches, saved to chunk_027.csv\n",
      "Processing chunk 28: rows 1350000-1400000 (50000 rows, 23.6% complete)\n",
      "  → Found 49461 matches, saved to chunk_028.csv\n",
      "Processing chunk 29: rows 1400000-1450000 (50000 rows, 24.4% complete)\n",
      "  → Found 35577 matches, saved to chunk_029.csv\n",
      "Processing chunk 30: rows 1450000-1500000 (50000 rows, 25.3% complete)\n",
      "  → Found 54634 matches, saved to chunk_030.csv\n",
      "Processing chunk 31: rows 1500000-1550000 (50000 rows, 26.1% complete)\n",
      "  → Found 51910 matches, saved to chunk_031.csv\n",
      "Processing chunk 32: rows 1550000-1600000 (50000 rows, 26.9% complete)\n",
      "  → Found 44525 matches, saved to chunk_032.csv\n",
      "Processing chunk 33: rows 1600000-1650000 (50000 rows, 27.8% complete)\n",
      "  → Found 48335 matches, saved to chunk_033.csv\n",
      "Processing chunk 34: rows 1650000-1700000 (50000 rows, 28.6% complete)\n",
      "  → Found 39819 matches, saved to chunk_034.csv\n",
      "Processing chunk 35: rows 1700000-1750000 (50000 rows, 29.5% complete)\n",
      "  → Found 49337 matches, saved to chunk_035.csv\n",
      "Processing chunk 36: rows 1750000-1800000 (50000 rows, 30.3% complete)\n",
      "  → Found 48434 matches, saved to chunk_036.csv\n",
      "Processing chunk 37: rows 1800000-1850000 (50000 rows, 31.2% complete)\n",
      "  → Found 50075 matches, saved to chunk_037.csv\n",
      "Processing chunk 38: rows 1850000-1900000 (50000 rows, 32.0% complete)\n",
      "  → Found 33463 matches, saved to chunk_038.csv\n",
      "Processing chunk 39: rows 1900000-1950000 (50000 rows, 32.8% complete)\n",
      "  → Found 50829 matches, saved to chunk_039.csv\n",
      "Processing chunk 40: rows 1950000-2000000 (50000 rows, 33.7% complete)\n",
      "  → Found 45620 matches, saved to chunk_040.csv\n",
      "Processing chunk 41: rows 2000000-2050000 (50000 rows, 34.5% complete)\n",
      "  → Found 49767 matches, saved to chunk_041.csv\n",
      "Processing chunk 42: rows 2050000-2100000 (50000 rows, 35.4% complete)\n",
      "  → Found 58106 matches, saved to chunk_042.csv\n",
      "Processing chunk 43: rows 2100000-2150000 (50000 rows, 36.2% complete)\n",
      "  → Found 43318 matches, saved to chunk_043.csv\n",
      "Processing chunk 44: rows 2150000-2200000 (50000 rows, 37.0% complete)\n",
      "  → Found 36180 matches, saved to chunk_044.csv\n",
      "Processing chunk 45: rows 2200000-2250000 (50000 rows, 37.9% complete)\n",
      "  → Found 45071 matches, saved to chunk_045.csv\n",
      "Processing chunk 46: rows 2250000-2300000 (50000 rows, 38.7% complete)\n",
      "  → Found 55439 matches, saved to chunk_046.csv\n",
      "Processing chunk 47: rows 2300000-2350000 (50000 rows, 39.6% complete)\n",
      "  → Found 57312 matches, saved to chunk_047.csv\n",
      "Processing chunk 48: rows 2350000-2400000 (50000 rows, 40.4% complete)\n",
      "  → Found 49800 matches, saved to chunk_048.csv\n",
      "Processing chunk 49: rows 2400000-2450000 (50000 rows, 41.3% complete)\n",
      "  → Found 47580 matches, saved to chunk_049.csv\n",
      "Processing chunk 50: rows 2450000-2500000 (50000 rows, 42.1% complete)\n",
      "  → Found 45376 matches, saved to chunk_050.csv\n",
      "Processing chunk 51: rows 2500000-2550000 (50000 rows, 42.9% complete)\n",
      "  → Found 47392 matches, saved to chunk_051.csv\n",
      "Processing chunk 52: rows 2550000-2600000 (50000 rows, 43.8% complete)\n",
      "  → Found 44149 matches, saved to chunk_052.csv\n",
      "Processing chunk 53: rows 2600000-2650000 (50000 rows, 44.6% complete)\n",
      "  → Found 47498 matches, saved to chunk_053.csv\n",
      "Processing chunk 54: rows 2650000-2700000 (50000 rows, 45.5% complete)\n",
      "  → Found 47602 matches, saved to chunk_054.csv\n",
      "Processing chunk 55: rows 2700000-2750000 (50000 rows, 46.3% complete)\n",
      "  → Found 48008 matches, saved to chunk_055.csv\n",
      "Processing chunk 56: rows 2750000-2800000 (50000 rows, 47.1% complete)\n",
      "  → Found 45979 matches, saved to chunk_056.csv\n",
      "Processing chunk 57: rows 2800000-2850000 (50000 rows, 48.0% complete)\n",
      "  → Found 29029 matches, saved to chunk_057.csv\n",
      "Processing chunk 58: rows 2850000-2900000 (50000 rows, 48.8% complete)\n",
      "  → Found 52345 matches, saved to chunk_058.csv\n",
      "Processing chunk 59: rows 2900000-2950000 (50000 rows, 49.7% complete)\n",
      "  → Found 51098 matches, saved to chunk_059.csv\n",
      "Processing chunk 60: rows 2950000-3000000 (50000 rows, 50.5% complete)\n",
      "  → Found 49275 matches, saved to chunk_060.csv\n",
      "Processing chunk 61: rows 3000000-3050000 (50000 rows, 51.4% complete)\n",
      "  → Found 35482 matches, saved to chunk_061.csv\n",
      "Processing chunk 62: rows 3050000-3100000 (50000 rows, 52.2% complete)\n",
      "  → Found 46266 matches, saved to chunk_062.csv\n",
      "Processing chunk 63: rows 3100000-3150000 (50000 rows, 53.0% complete)\n",
      "  → Found 77416 matches, saved to chunk_063.csv\n",
      "Processing chunk 64: rows 3150000-3200000 (50000 rows, 53.9% complete)\n",
      "  → Found 54890 matches, saved to chunk_064.csv\n",
      "Processing chunk 65: rows 3200000-3250000 (50000 rows, 54.7% complete)\n",
      "  → Found 46927 matches, saved to chunk_065.csv\n",
      "Processing chunk 66: rows 3250000-3300000 (50000 rows, 55.6% complete)\n",
      "  → Found 34349 matches, saved to chunk_066.csv\n",
      "Processing chunk 67: rows 3300000-3350000 (50000 rows, 56.4% complete)\n",
      "  → Found 50720 matches, saved to chunk_067.csv\n",
      "Processing chunk 68: rows 3350000-3400000 (50000 rows, 57.3% complete)\n",
      "  → Found 58354 matches, saved to chunk_068.csv\n",
      "Processing chunk 69: rows 3400000-3450000 (50000 rows, 58.1% complete)\n",
      "  → Found 46528 matches, saved to chunk_069.csv\n",
      "Processing chunk 70: rows 3450000-3500000 (50000 rows, 58.9% complete)\n",
      "  → Found 46608 matches, saved to chunk_070.csv\n",
      "Processing chunk 71: rows 3500000-3550000 (50000 rows, 59.8% complete)\n",
      "  → Found 48796 matches, saved to chunk_071.csv\n",
      "Processing chunk 72: rows 3550000-3600000 (50000 rows, 60.6% complete)\n",
      "  → Found 50573 matches, saved to chunk_072.csv\n",
      "Processing chunk 73: rows 3600000-3650000 (50000 rows, 61.5% complete)\n",
      "  → Found 59559 matches, saved to chunk_073.csv\n",
      "Processing chunk 74: rows 3650000-3700000 (50000 rows, 62.3% complete)\n",
      "  → Found 46144 matches, saved to chunk_074.csv\n",
      "Processing chunk 75: rows 3700000-3750000 (50000 rows, 63.1% complete)\n",
      "  → Found 41967 matches, saved to chunk_075.csv\n",
      "Processing chunk 76: rows 3750000-3800000 (50000 rows, 64.0% complete)\n",
      "  → Found 41383 matches, saved to chunk_076.csv\n",
      "Processing chunk 77: rows 3800000-3850000 (50000 rows, 64.8% complete)\n",
      "  → Found 45300 matches, saved to chunk_077.csv\n",
      "Processing chunk 78: rows 3850000-3900000 (50000 rows, 65.7% complete)\n",
      "  → Found 49477 matches, saved to chunk_078.csv\n",
      "Processing chunk 79: rows 3900000-3950000 (50000 rows, 66.5% complete)\n",
      "  → Found 38611 matches, saved to chunk_079.csv\n",
      "Processing chunk 80: rows 3950000-4000000 (50000 rows, 67.4% complete)\n",
      "  → Found 38529 matches, saved to chunk_080.csv\n",
      "Processing chunk 81: rows 4000000-4050000 (50000 rows, 68.2% complete)\n",
      "  → Found 57516 matches, saved to chunk_081.csv\n",
      "Processing chunk 82: rows 4050000-4100000 (50000 rows, 69.0% complete)\n",
      "  → Found 46296 matches, saved to chunk_082.csv\n",
      "Processing chunk 83: rows 4100000-4150000 (50000 rows, 69.9% complete)\n",
      "  → Found 39615 matches, saved to chunk_083.csv\n",
      "Processing chunk 84: rows 4150000-4200000 (50000 rows, 70.7% complete)\n",
      "  → Found 48212 matches, saved to chunk_084.csv\n",
      "Processing chunk 85: rows 4200000-4250000 (50000 rows, 71.6% complete)\n",
      "  → Found 43545 matches, saved to chunk_085.csv\n",
      "Processing chunk 86: rows 4250000-4300000 (50000 rows, 72.4% complete)\n",
      "  → Found 42948 matches, saved to chunk_086.csv\n",
      "Processing chunk 87: rows 4300000-4350000 (50000 rows, 73.2% complete)\n",
      "  → Found 41484 matches, saved to chunk_087.csv\n",
      "Processing chunk 88: rows 4350000-4400000 (50000 rows, 74.1% complete)\n",
      "  → Found 42047 matches, saved to chunk_088.csv\n",
      "Processing chunk 89: rows 4400000-4450000 (50000 rows, 74.9% complete)\n",
      "  → Found 50224 matches, saved to chunk_089.csv\n",
      "Processing chunk 90: rows 4450000-4500000 (50000 rows, 75.8% complete)\n",
      "  → Found 39871 matches, saved to chunk_090.csv\n",
      "Processing chunk 91: rows 4500000-4550000 (50000 rows, 76.6% complete)\n",
      "  → Found 76002 matches, saved to chunk_091.csv\n",
      "Processing chunk 92: rows 4550000-4600000 (50000 rows, 77.5% complete)\n",
      "  → Found 53178 matches, saved to chunk_092.csv\n",
      "Processing chunk 93: rows 4600000-4650000 (50000 rows, 78.3% complete)\n",
      "  → Found 47963 matches, saved to chunk_093.csv\n",
      "Processing chunk 94: rows 4650000-4700000 (50000 rows, 79.1% complete)\n",
      "  → Found 46904 matches, saved to chunk_094.csv\n",
      "Processing chunk 95: rows 4700000-4750000 (50000 rows, 80.0% complete)\n",
      "  → Found 49023 matches, saved to chunk_095.csv\n",
      "Processing chunk 96: rows 4750000-4800000 (50000 rows, 80.8% complete)\n",
      "  → Found 38347 matches, saved to chunk_096.csv\n",
      "Processing chunk 97: rows 4800000-4850000 (50000 rows, 81.7% complete)\n",
      "  → Found 46782 matches, saved to chunk_097.csv\n",
      "Processing chunk 98: rows 4850000-4900000 (50000 rows, 82.5% complete)\n",
      "  → Found 41443 matches, saved to chunk_098.csv\n",
      "Processing chunk 99: rows 4900000-4950000 (50000 rows, 83.4% complete)\n",
      "  → Found 49277 matches, saved to chunk_099.csv\n",
      "Processing chunk 100: rows 4950000-5000000 (50000 rows, 84.2% complete)\n",
      "  → Found 41923 matches, saved to chunk_100.csv\n",
      "Processing chunk 101: rows 5000000-5050000 (50000 rows, 85.0% complete)\n",
      "  → Found 50355 matches, saved to chunk_101.csv\n",
      "Processing chunk 102: rows 5050000-5100000 (50000 rows, 85.9% complete)\n",
      "  → Found 44016 matches, saved to chunk_102.csv\n",
      "Processing chunk 103: rows 5100000-5150000 (50000 rows, 86.7% complete)\n",
      "  → Found 41105 matches, saved to chunk_103.csv\n",
      "Processing chunk 104: rows 5150000-5200000 (50000 rows, 87.6% complete)\n",
      "  → Found 36234 matches, saved to chunk_104.csv\n",
      "Processing chunk 105: rows 5200000-5250000 (50000 rows, 88.4% complete)\n",
      "  → Found 54673 matches, saved to chunk_105.csv\n",
      "Processing chunk 106: rows 5250000-5300000 (50000 rows, 89.2% complete)\n",
      "  → Found 74316 matches, saved to chunk_106.csv\n",
      "Processing chunk 107: rows 5300000-5350000 (50000 rows, 90.1% complete)\n",
      "  → Found 40224 matches, saved to chunk_107.csv\n",
      "Processing chunk 108: rows 5350000-5400000 (50000 rows, 90.9% complete)\n",
      "  → Found 72307 matches, saved to chunk_108.csv\n",
      "Processing chunk 109: rows 5400000-5450000 (50000 rows, 91.8% complete)\n",
      "  → Found 53508 matches, saved to chunk_109.csv\n",
      "Processing chunk 110: rows 5450000-5500000 (50000 rows, 92.6% complete)\n",
      "  → Found 41087 matches, saved to chunk_110.csv\n",
      "Processing chunk 111: rows 5500000-5550000 (50000 rows, 93.5% complete)\n",
      "  → Found 31839 matches, saved to chunk_111.csv\n",
      "Processing chunk 112: rows 5550000-5600000 (50000 rows, 94.3% complete)\n",
      "  → Found 29099 matches, saved to chunk_112.csv\n",
      "Processing chunk 113: rows 5600000-5650000 (50000 rows, 95.1% complete)\n",
      "  → Found 35272 matches, saved to chunk_113.csv\n",
      "Processing chunk 114: rows 5650000-5700000 (50000 rows, 96.0% complete)\n",
      "  → Found 37459 matches, saved to chunk_114.csv\n",
      "Processing chunk 115: rows 5700000-5750000 (50000 rows, 96.8% complete)\n",
      "  → Found 32978 matches, saved to chunk_115.csv\n",
      "Processing chunk 116: rows 5750000-5800000 (50000 rows, 97.7% complete)\n",
      "  → Found 21679 matches, saved to chunk_116.csv\n",
      "Processing chunk 117: rows 5800000-5850000 (50000 rows, 98.5% complete)\n",
      "  → Found 17342 matches, saved to chunk_117.csv\n",
      "Processing chunk 118: rows 5850000-5900000 (50000 rows, 99.3% complete)\n",
      "  → Found 74428 matches, saved to chunk_118.csv\n",
      "Processing chunk 119: rows 5900000-5938627 (38627 rows, 100.0% complete)\n",
      "  → Found 63056 matches, saved to chunk_119.csv\n",
      "\n",
      "Processing complete! Total matches found: 5510232\n"
     ]
    }
   ],
   "source": [
    "def process_company_matches_memory_efficient(df_company_matches, df_patstat_person_id, \n",
    "                                           chunk_size=50000, output_dir=None):\n",
    "    \"\"\"\n",
    "    Process company matches in small chunks to avoid memory issues\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = '/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_cb_patents/'\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_matches = 0\n",
    "    total_rows = len(df_company_matches)\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i, chunk_start in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_rows)\n",
    "        \n",
    "        print(f\"Processing chunk {i+1}: rows {chunk_start}-{chunk_end} \"\n",
    "              f\"({chunk_end-chunk_start} rows, {(chunk_end/total_rows)*100:.1f}% complete)\")\n",
    "        \n",
    "        # Get chunk - use .copy() to ensure we have a clean copy\n",
    "        chunk = df_company_matches.iloc[chunk_start:chunk_end].copy()\n",
    "        \n",
    "        # Perform merge\n",
    "        try:\n",
    "            matches = pd.merge(chunk, df_patstat_person_id, \n",
    "                             left_on='ps_person_id', right_on='person_id', \n",
    "                             how='inner')\n",
    "            \n",
    "            # Remove duplicates\n",
    "            matches.drop_duplicates(subset=['ps_person_id', 'appln_id'], inplace=True)\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                # Save immediately\n",
    "                output_file = f'{output_dir}company_matches_patstat_chunk_{i+1:03d}.csv'\n",
    "                matches.to_csv(output_file, index=False)\n",
    "                total_matches += len(matches)\n",
    "                print(f\"  → Found {len(matches)} matches, saved to chunk_{i+1:03d}.csv\")\n",
    "            else:\n",
    "                print(f\"  → No matches found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  → Error processing chunk {i+1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Explicit cleanup\n",
    "        del chunk, matches\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nProcessing complete! Total matches found: {total_matches}\")\n",
    "    return total_matches\n",
    "\n",
    "# Use the function\n",
    "total_matches = process_company_matches_memory_efficient(\n",
    "    df_company_matches, \n",
    "    df_patstat_person_id, \n",
    "    chunk_size=50000  # Start with 50k rows, adjust if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b3e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 119 chunk files into /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_cb_patents/company_matches_patstat_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to combine all chunk files into one final CSV file\n",
    "def combine_chunk_files(output_dir):\n",
    "    \"\"\"\n",
    "    Combine all chunk files into one final CSV file\n",
    "    \"\"\"\n",
    "    chunk_files = glob.glob(f\"{output_dir}company_matches_patstat_chunk_*.csv\")\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(\"No chunk files found to combine.\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat((pd.read_csv(f) for f in chunk_files), ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=['ps_person_id', 'appln_id'], inplace=True)\n",
    "    \n",
    "    final_output_file = f\"{output_dir}company_matches_patstat_combined.csv\"\n",
    "    combined_df.to_csv(final_output_file, index=False)\n",
    "    \n",
    "    print(f\"Combined {len(chunk_files)} chunk files into {final_output_file}\")\n",
    "    return combined_df\n",
    "\n",
    "df_company_matches_patstat_combined = combine_chunk_files('/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/matching_cb_patents/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7adb23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_matches_patstat_combined['appln_id'] = df_company_matches_patstat_combined['appln_id'].astype(str)\n",
    "df_patstat_cleantech['appln_id'] = df_patstat_cleantech['appln_id'].astype(str)\n",
    "df_company_matches_patstat_combined = df_company_matches_patstat_combined.merge(df_patstat_cleantech[['appln_id', 'ipc_class_symbol', 'cpc_class_symbol']], on='appln_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa24e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_matches_patstat['appln_id'] = df_people_matches_patstat['appln_id'].astype(str)\n",
    "df_person_matches_patstat_combined = pd.merge(df_people_matches_patstat, df_patstat_cleantech[['appln_id', 'ipc_class_symbol', 'cpc_class_symbol']], on='appln_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054a81c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162917, 400778)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_company_matches_patstat_combined), len(df_person_matches_patstat_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b80cc12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cb_uuid</th>\n",
       "      <th>cb_name</th>\n",
       "      <th>cb_harmonized</th>\n",
       "      <th>ps_person_id</th>\n",
       "      <th>ps_name</th>\n",
       "      <th>ps_harmonized</th>\n",
       "      <th>appln_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>ipc_class_symbol</th>\n",
       "      <th>cpc_class_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ceca97b-493c-1446-6249-5aaa33464763</td>\n",
       "      <td>Kevin Flaherty</td>\n",
       "      <td>KEVIN FLAHERTY</td>\n",
       "      <td>45307436</td>\n",
       "      <td>Flaherty, Kevin</td>\n",
       "      <td>FLAHERTY KEVIN</td>\n",
       "      <td>411568025</td>\n",
       "      <td>45307436</td>\n",
       "      <td>[H01L  31/042, H01L  31/052, H01L  31/18]</td>\n",
       "      <td>[Y02E  10/47, Y02E  10/50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b598c59-7b6c-2d48-763c-da55bca77035</td>\n",
       "      <td>Owen Byrne</td>\n",
       "      <td>OWEN BY</td>\n",
       "      <td>48467086</td>\n",
       "      <td>Owen, Byron G.</td>\n",
       "      <td>OWEN BYRON</td>\n",
       "      <td>448273380</td>\n",
       "      <td>48467086</td>\n",
       "      <td>[E21B  43/16]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01b8d46-d311-3333-7c34-aa3ae9c03f22</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>MARK ZUCKERBERG</td>\n",
       "      <td>11063428</td>\n",
       "      <td>Zuckerberg, Mark E.</td>\n",
       "      <td>ZUCKERBERG MARK</td>\n",
       "      <td>364671708</td>\n",
       "      <td>11063428</td>\n",
       "      <td>[G06Q  50/00]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01b8d46-d311-3333-7c34-aa3ae9c03f22</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>MARK ZUCKERBERG</td>\n",
       "      <td>11063428</td>\n",
       "      <td>Zuckerberg, Mark E.</td>\n",
       "      <td>ZUCKERBERG MARK</td>\n",
       "      <td>331523353</td>\n",
       "      <td>11063428</td>\n",
       "      <td>[G06Q  30/00, G06Q  30/02]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01b8d46-d311-3333-7c34-aa3ae9c03f22</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>MARK ZUCKERBERG</td>\n",
       "      <td>11063428</td>\n",
       "      <td>Zuckerberg, Mark E.</td>\n",
       "      <td>ZUCKERBERG MARK</td>\n",
       "      <td>421488729</td>\n",
       "      <td>11063428</td>\n",
       "      <td>[G06Q  50/00]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                cb_uuid          cb_name    cb_harmonized  \\\n",
       "0  5ceca97b-493c-1446-6249-5aaa33464763   Kevin Flaherty   KEVIN FLAHERTY   \n",
       "1  3b598c59-7b6c-2d48-763c-da55bca77035       Owen Byrne          OWEN BY   \n",
       "2  a01b8d46-d311-3333-7c34-aa3ae9c03f22  Mark Zuckerberg  MARK ZUCKERBERG   \n",
       "3  a01b8d46-d311-3333-7c34-aa3ae9c03f22  Mark Zuckerberg  MARK ZUCKERBERG   \n",
       "4  a01b8d46-d311-3333-7c34-aa3ae9c03f22  Mark Zuckerberg  MARK ZUCKERBERG   \n",
       "\n",
       "   ps_person_id              ps_name    ps_harmonized   appln_id  person_id  \\\n",
       "0      45307436      Flaherty, Kevin   FLAHERTY KEVIN  411568025   45307436   \n",
       "1      48467086       Owen, Byron G.       OWEN BYRON  448273380   48467086   \n",
       "2      11063428  Zuckerberg, Mark E.  ZUCKERBERG MARK  364671708   11063428   \n",
       "3      11063428  Zuckerberg, Mark E.  ZUCKERBERG MARK  331523353   11063428   \n",
       "4      11063428  Zuckerberg, Mark E.  ZUCKERBERG MARK  421488729   11063428   \n",
       "\n",
       "                            ipc_class_symbol            cpc_class_symbol  \n",
       "0  [H01L  31/042, H01L  31/052, H01L  31/18]  [Y02E  10/47, Y02E  10/50]  \n",
       "1                              [E21B  43/16]                      [None]  \n",
       "2                              [G06Q  50/00]                      [None]  \n",
       "3                 [G06Q  30/00, G06Q  30/02]                      [None]  \n",
       "4                              [G06Q  50/00]                      [None]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_person_matches_patstat_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96b277",
   "metadata": {},
   "source": [
    "# Match with Crunchbase Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da3d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cb_funding_rounds = pd.read_csv('/mnt/hdd02/Projekt_EDV_TEK/CrunchBase Dump TUHH/funding_rounds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ecb162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
       "       'updated_at', 'country_code', 'state_code', 'region', 'city',\n",
       "       'investment_type', 'announced_on', 'raised_amount_usd', 'raised_amount',\n",
       "       'raised_amount_currency_code', 'post_money_valuation_usd',\n",
       "       'post_money_valuation', 'post_money_valuation_currency_code',\n",
       "       'investor_count', 'org_uuid', 'org_name', 'lead_investor_uuids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb_funding_rounds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916394c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>permalink</th>\n",
       "      <th>cb_url</th>\n",
       "      <th>rank</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>...</th>\n",
       "      <th>raised_amount_usd</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_amount_currency_code</th>\n",
       "      <th>post_money_valuation_usd</th>\n",
       "      <th>post_money_valuation</th>\n",
       "      <th>post_money_valuation_currency_code</th>\n",
       "      <th>investor_count</th>\n",
       "      <th>org_uuid</th>\n",
       "      <th>org_name</th>\n",
       "      <th>lead_investor_uuids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a945939-18e0-cc9d-27b9-bf33817b2818</td>\n",
       "      <td>Angel Round - Meta</td>\n",
       "      <td>funding_round</td>\n",
       "      <td>facebook-angel--8a945939</td>\n",
       "      <td>https://www.crunchbase.com/funding_round/faceb...</td>\n",
       "      <td>393543.0</td>\n",
       "      <td>2007-05-27 06:08:18</td>\n",
       "      <td>2018-02-12 23:05:39</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
       "      <td>Meta</td>\n",
       "      <td>3f47be49-2e32-8118-01a0-31685a4d0fd7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d950d7a5-79ff-fb93-ca87-13386b0e2feb</td>\n",
       "      <td>Series A - Meta</td>\n",
       "      <td>funding_round</td>\n",
       "      <td>facebook-series-a--d950d7a5</td>\n",
       "      <td>https://www.crunchbase.com/funding_round/faceb...</td>\n",
       "      <td>307991.0</td>\n",
       "      <td>2007-05-27 06:09:10</td>\n",
       "      <td>2018-02-12 23:52:16</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>12700000.0</td>\n",
       "      <td>12700000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>98000000.0</td>\n",
       "      <td>98000000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>4.0</td>\n",
       "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
       "      <td>Meta</td>\n",
       "      <td>b08efc27-da40-505a-6f9d-c9e14247bf36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6fae3958-a001-27c0-fb7e-666266aedd78</td>\n",
       "      <td>Series B - Meta</td>\n",
       "      <td>funding_round</td>\n",
       "      <td>facebook-series-b--6fae3958</td>\n",
       "      <td>https://www.crunchbase.com/funding_round/faceb...</td>\n",
       "      <td>108049.0</td>\n",
       "      <td>2007-05-27 06:09:36</td>\n",
       "      <td>2018-02-12 23:30:46</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>27500000.0</td>\n",
       "      <td>27500000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>502500000.0</td>\n",
       "      <td>502500000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>5.0</td>\n",
       "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
       "      <td>Meta</td>\n",
       "      <td>e2006571-6b7a-e477-002a-f7014f48a7e3,8d5c7e48-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcd5a63d-ed99-6963-0dd2-e36f6582f846</td>\n",
       "      <td>Series B - Photobucket</td>\n",
       "      <td>funding_round</td>\n",
       "      <td>photobucket-series-b--bcd5a63d</td>\n",
       "      <td>https://www.crunchbase.com/funding_round/photo...</td>\n",
       "      <td>469186.0</td>\n",
       "      <td>2007-05-29 11:05:59</td>\n",
       "      <td>2018-02-12 23:27:36</td>\n",
       "      <td>USA</td>\n",
       "      <td>CO</td>\n",
       "      <td>...</td>\n",
       "      <td>10500000.0</td>\n",
       "      <td>10500000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>f53cb4de-236e-0b1b-dee8-7104a8b018f9</td>\n",
       "      <td>Photobucket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60e6afd9-1215-465a-dd17-0ed600d4e29b</td>\n",
       "      <td>Series A - Geni</td>\n",
       "      <td>funding_round</td>\n",
       "      <td>geni-series-a--60e6afd9</td>\n",
       "      <td>https://www.crunchbase.com/funding_round/geni-...</td>\n",
       "      <td>497495.0</td>\n",
       "      <td>2007-05-31 20:19:28</td>\n",
       "      <td>2018-02-12 23:41:29</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4111dc8b-c0df-2d24-ed33-30cd137b3098</td>\n",
       "      <td>Geni</td>\n",
       "      <td>fb2f8884-ec07-895a-48d7-d9a9d4d7175c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid                    name  \\\n",
       "0  8a945939-18e0-cc9d-27b9-bf33817b2818      Angel Round - Meta   \n",
       "1  d950d7a5-79ff-fb93-ca87-13386b0e2feb         Series A - Meta   \n",
       "2  6fae3958-a001-27c0-fb7e-666266aedd78         Series B - Meta   \n",
       "3  bcd5a63d-ed99-6963-0dd2-e36f6582f846  Series B - Photobucket   \n",
       "4  60e6afd9-1215-465a-dd17-0ed600d4e29b         Series A - Geni   \n",
       "\n",
       "            type                       permalink  \\\n",
       "0  funding_round        facebook-angel--8a945939   \n",
       "1  funding_round     facebook-series-a--d950d7a5   \n",
       "2  funding_round     facebook-series-b--6fae3958   \n",
       "3  funding_round  photobucket-series-b--bcd5a63d   \n",
       "4  funding_round         geni-series-a--60e6afd9   \n",
       "\n",
       "                                              cb_url      rank  \\\n",
       "0  https://www.crunchbase.com/funding_round/faceb...  393543.0   \n",
       "1  https://www.crunchbase.com/funding_round/faceb...  307991.0   \n",
       "2  https://www.crunchbase.com/funding_round/faceb...  108049.0   \n",
       "3  https://www.crunchbase.com/funding_round/photo...  469186.0   \n",
       "4  https://www.crunchbase.com/funding_round/geni-...  497495.0   \n",
       "\n",
       "            created_at           updated_at country_code state_code  ...  \\\n",
       "0  2007-05-27 06:08:18  2018-02-12 23:05:39          USA         CA  ...   \n",
       "1  2007-05-27 06:09:10  2018-02-12 23:52:16          USA         CA  ...   \n",
       "2  2007-05-27 06:09:36  2018-02-12 23:30:46          USA         CA  ...   \n",
       "3  2007-05-29 11:05:59  2018-02-12 23:27:36          USA         CO  ...   \n",
       "4  2007-05-31 20:19:28  2018-02-12 23:41:29          USA         CA  ...   \n",
       "\n",
       "  raised_amount_usd raised_amount raised_amount_currency_code  \\\n",
       "0          500000.0      500000.0                         USD   \n",
       "1        12700000.0    12700000.0                         USD   \n",
       "2        27500000.0    27500000.0                         USD   \n",
       "3        10500000.0    10500000.0                         USD   \n",
       "4               NaN           NaN                         NaN   \n",
       "\n",
       "  post_money_valuation_usd  post_money_valuation  \\\n",
       "0                      NaN                   NaN   \n",
       "1               98000000.0            98000000.0   \n",
       "2              502500000.0           502500000.0   \n",
       "3                      NaN                   NaN   \n",
       "4               10000000.0            10000000.0   \n",
       "\n",
       "   post_money_valuation_currency_code investor_count  \\\n",
       "0                                 NaN            4.0   \n",
       "1                                 USD            4.0   \n",
       "2                                 USD            5.0   \n",
       "3                                 NaN            2.0   \n",
       "4                                 USD            1.0   \n",
       "\n",
       "                               org_uuid     org_name  \\\n",
       "0  df662812-7f97-0b43-9d3e-12f64f504fbb         Meta   \n",
       "1  df662812-7f97-0b43-9d3e-12f64f504fbb         Meta   \n",
       "2  df662812-7f97-0b43-9d3e-12f64f504fbb         Meta   \n",
       "3  f53cb4de-236e-0b1b-dee8-7104a8b018f9  Photobucket   \n",
       "4  4111dc8b-c0df-2d24-ed33-30cd137b3098         Geni   \n",
       "\n",
       "                                 lead_investor_uuids  \n",
       "0               3f47be49-2e32-8118-01a0-31685a4d0fd7  \n",
       "1               b08efc27-da40-505a-6f9d-c9e14247bf36  \n",
       "2  e2006571-6b7a-e477-002a-f7014f48a7e3,8d5c7e48-...  \n",
       "3                                                NaN  \n",
       "4               fb2f8884-ec07-895a-48d7-d9a9d4d7175c  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb_funding_rounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdaf518",
   "metadata": {},
   "source": [
    "# Match Patent Data with Funding Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29e60ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching company patents with funding rounds...\n",
      "Company patents before matching: 1162917\n",
      "Funding rounds available: 545920\n",
      "Company patents after matching: 467532\n",
      "Unique companies with funding data: 17608\n",
      "\n",
      "Funding rounds per company statistics:\n",
      "count    17608.000000\n",
      "mean         2.880850\n",
      "std          2.649838\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max         33.000000\n",
      "Name: uuid, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Match company patents with funding rounds (one-to-many)\n",
    "print(\"Matching company patents with funding rounds...\")\n",
    "print(f\"Company patents before matching: {len(df_company_matches_patstat_combined)}\")\n",
    "print(f\"Funding rounds available: {len(df_cb_funding_rounds)}\")\n",
    "\n",
    "# Perform the merge - this will create multiple rows for companies with multiple funding rounds\n",
    "df_company_patents_funding = pd.merge(\n",
    "    df_company_matches_patstat_combined, \n",
    "    df_cb_funding_rounds, \n",
    "    left_on='cb_uuid', \n",
    "    right_on='org_uuid',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Company patents after matching: {len(df_company_patents_funding)}\")\n",
    "print(f\"Unique companies with funding data: {df_company_patents_funding['cb_uuid'].nunique()}\")\n",
    "\n",
    "# Check how many funding rounds per company\n",
    "funding_per_company = df_company_patents_funding.groupby('cb_uuid')['uuid'].nunique().describe()\n",
    "print(f\"\\nFunding rounds per company statistics:\")\n",
    "print(funding_per_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "230a8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPANY PATENTS WITH FUNDING DATA ===\n",
      "Total records: 467532\n",
      "Records with funding data: 467532\n",
      "\n",
      "Sample of company patents with funding:\n",
      "  cb_name       ps_name   appln_id           type  raised_amount_usd           created_at\n",
      "0  Scribd  Scribd, Inc.  577867352  funding_round            40000.0  2007-06-02 07:36:21\n",
      "1  Scribd  Scribd, Inc.  577867352  funding_round          3710000.0  2007-06-02 07:49:08\n",
      "2  Scribd  Scribd, Inc.  577867352  funding_round          9000000.0  2008-12-19 09:01:24\n",
      "3  Scribd  Scribd, Inc.  577867352  funding_round         13000000.0  2011-01-20 05:02:12\n",
      "4  Scribd  Scribd, Inc.  577867352  funding_round         22000000.0  2015-01-05 09:31:05\n"
     ]
    }
   ],
   "source": [
    "# Explore the matched data\n",
    "print(\"=== COMPANY PATENTS WITH FUNDING DATA ===\")\n",
    "print(f\"Total records: {len(df_company_patents_funding)}\")\n",
    "print(f\"Records with funding data: {len(df_company_patents_funding[df_company_patents_funding['uuid'].notna()])}\")\n",
    "\n",
    "# Show sample of matched data\n",
    "print(\"\\nSample of company patents with funding:\")\n",
    "sample_with_funding = df_company_patents_funding[df_company_patents_funding['uuid'].notna()].head()\n",
    "print(sample_with_funding[['cb_name', 'ps_name', 'appln_id', 'type', 'raised_amount_usd', 'created_at']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead020fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved company patents with funding data: 467532 records\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total unique companies with patents: 17608\n",
      "Total unique patents (company): 152084\n",
      "\n",
      "Funding amount statistics (Company patents):\n",
      "  Total funding rounds: 377773\n",
      "  Total amount raised: $220,835,448,085,589.00\n",
      "  Average funding per round: $584,571,814.52\n",
      "  Median funding per round: $9,000,000.00\n"
     ]
    }
   ],
   "source": [
    "# Save the matched results\n",
    "output_dir = '/home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save company patents with funding data\n",
    "df_company_patents_funding.to_csv(f'{output_dir}company_patents_funding_matched.csv', index=False)\n",
    "print(f\"Saved company patents with funding data: {len(df_company_patents_funding)} records\")\n",
    "\n",
    "# Create summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total unique companies with patents: {df_company_patents_funding['cb_uuid'].nunique()}\")\n",
    "print(f\"Total unique patents (company): {df_company_patents_funding['appln_id'].nunique()}\")\n",
    "\n",
    "# Funding statistics\n",
    "company_funding_stats = df_company_patents_funding[df_company_patents_funding['raised_amount_usd'].notna()]['raised_amount_usd']\n",
    "\n",
    "print(f\"\\nFunding amount statistics (Company patents):\")\n",
    "print(f\"  Total funding rounds: {len(company_funding_stats)}\")\n",
    "print(f\"  Total amount raised: ${company_funding_stats.sum():,.2f}\")\n",
    "print(f\"  Average funding per round: ${company_funding_stats.mean():,.2f}\")\n",
    "print(f\"  Median funding per round: ${company_funding_stats.median():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f56778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUNDING TYPE ANALYSIS ===\n",
      "Company Patents - Funding Types:\n",
      "type\n",
      "funding_round    467532\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== TEMPORAL FUNDING ANALYSIS ===\n",
      "Company Patents - Funding by Year (top 10):\n",
      "funding_year\n",
      "2021    52484\n",
      "2017    48753\n",
      "2022    41329\n",
      "2014    37338\n",
      "2020    37080\n",
      "2015    36359\n",
      "2013    33521\n",
      "2016    33469\n",
      "2018    33382\n",
      "2019    29071\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze funding types and patterns\n",
    "print(\"=== FUNDING TYPE ANALYSIS ===\")\n",
    "\n",
    "# Company funding types\n",
    "print(\"Company Patents - Funding Types:\")\n",
    "company_funding_types = df_company_patents_funding[df_company_patents_funding['type'].notna()]['type'].value_counts()\n",
    "print(company_funding_types.head(10))\n",
    "\n",
    "# Temporal analysis - extract year from created_at\n",
    "df_company_patents_funding['funding_year'] = pd.to_datetime(df_company_patents_funding['created_at']).dt.year\n",
    "\n",
    "print(\"\\n=== TEMPORAL FUNDING ANALYSIS ===\")\n",
    "print(\"Company Patents - Funding by Year (top 10):\")\n",
    "company_yearly = df_company_patents_funding[df_company_patents_funding['funding_year'].notna()]['funding_year'].value_counts().head(10)\n",
    "print(company_yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605bcb6",
   "metadata": {},
   "source": [
    "# TEK Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026e2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEK companies with Y02 codes: 122456\n",
      "Unique Y02 classes found: {'Y02T', 'Y02E', 'Y02C', 'Y02W', 'Y02B', 'Y02A', 'Y02P', 'Y02D'}\n"
     ]
    }
   ],
   "source": [
    "# CPC class symbols sind als Listen gespeichert - first convert from string to actual lists\n",
    "def safe_eval_list(x):\n",
    "    try:\n",
    "        if pd.isna(x) or x == '[None]' or x == 'None':\n",
    "            return []\n",
    "        if isinstance(x, str):\n",
    "            # Remove [None] entries and convert to list\n",
    "            cleaned = x.replace('[None]', '').replace('None', '').strip('[]')\n",
    "            if not cleaned:\n",
    "                return []\n",
    "            # Split by comma and clean up\n",
    "            items = [item.strip().strip(\"'\\\"\") for item in cleaned.split(',') if item.strip() and item.strip() != 'None']\n",
    "            return items\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return [item for item in x if item and item != 'None']\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Extract Y02 classes from CPC class symbols\n",
    "df_company_patents_funding['cpc_classes'] = df_company_patents_funding['cpc_class_symbol'].apply(safe_eval_list)\n",
    "\n",
    "# Extract Y02 codes specifically\n",
    "def extract_y02_codes(cpc_list):\n",
    "    if not cpc_list:\n",
    "        return []\n",
    "    y02_codes = []\n",
    "    for code in cpc_list:\n",
    "        if isinstance(code, str) and code.startswith('Y02'):\n",
    "            # Extract Y02 + next letter (e.g., Y02E, Y02T, etc.)\n",
    "            y02_main = code[:4]  # Y02E, Y02T, etc.\n",
    "            y02_codes.append(y02_main)\n",
    "    return list(set(y02_codes))  # Remove duplicates\n",
    "\n",
    "df_company_patents_funding['y02_classes'] = df_company_patents_funding['cpc_classes'].apply(extract_y02_codes)\n",
    "\n",
    "# Filter only companies with Y02 codes (TEKs)\n",
    "df_tek_funding = df_company_patents_funding[df_company_patents_funding['y02_classes'].apply(len) > 0].copy()\n",
    "\n",
    "print(f\"TEK companies with Y02 codes: {len(df_tek_funding)}\")\n",
    "print(f\"Unique Y02 classes found: {set([code for codes in df_tek_funding['y02_classes'] for code in codes])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b157ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded TEK data: 122456 records\n",
      "Y02 class distribution:\n",
      "y02_classes\n",
      "Y02E    40859\n",
      "Y02A    32425\n",
      "Y02D    15863\n",
      "Y02P    14316\n",
      "Y02B    10747\n",
      "Y02T     6432\n",
      "Y02W     1123\n",
      "Y02C      691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explode Y02 classes so each row represents one company-patent-Y02class combination\n",
    "df_tek_exploded = df_tek_funding.explode('y02_classes').copy()\n",
    "df_tek_exploded = df_tek_exploded[df_tek_exploded['y02_classes'].notna()]\n",
    "\n",
    "print(f\"Exploded TEK data: {len(df_tek_exploded)} records\")\n",
    "print(\"Y02 class distribution:\")\n",
    "print(df_tek_exploded['y02_classes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1457bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded TEK data: 122456 records\n",
      "Y02 class distribution:\n",
      "y02_classes\n",
      "Y02E    40859\n",
      "Y02A    32425\n",
      "Y02D    15863\n",
      "Y02P    14316\n",
      "Y02B    10747\n",
      "Y02T     6432\n",
      "Y02W     1123\n",
      "Y02C      691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explode Y02 classes so each row represents one company-patent-Y02class combination\n",
    "df_tek_exploded = df_tek_funding.explode('y02_classes').copy()\n",
    "df_tek_exploded = df_tek_exploded[df_tek_exploded['y02_classes'].notna()]\n",
    "\n",
    "print(f\"Exploded TEK data: {len(df_tek_exploded)} records\")\n",
    "print(\"Y02 class distribution:\")\n",
    "print(df_tek_exploded['y02_classes'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0ac4f",
   "metadata": {},
   "source": [
    "# Patent Novelty Analysis\n",
    "\n",
    "Now we'll analyze the same patent-funding matching data but categorize patents by their novelty scores (High, Medium, Low novelty) to understand how patent novelty relates to funding patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0cd2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty data loaded: 318104 patents\n",
      "TEK funding with novelty: 122456 records\n",
      "Novelty category distribution:\n",
      "novelty_category\n",
      "Medium    35865\n",
      "High      12080\n",
      "Low        2097\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if novelty data is available (from your AP 2)\n",
    "novelty_file_path = '/home/thiesen/Documents/Projekt_EDV-TEK/AP 3 - Diffusion von TEKs/edv_tek_diffusion_patent_novelty_scores.csv'\n",
    "\n",
    "try:\n",
    "    df_novelty = pd.read_csv(novelty_file_path)\n",
    "    print(f\"Novelty data loaded: {len(df_novelty)} patents\")\n",
    "    \n",
    "    # Create novelty categories\n",
    "    df_novelty['novelty_q100_percentile'] = df_novelty['novelty_q100'].rank(pct=True) * 100\n",
    "    df_novelty['novelty_category'] = 'Medium'\n",
    "    df_novelty.loc[df_novelty['novelty_q100_percentile'] >= 90, 'novelty_category'] = 'High'\n",
    "    df_novelty.loc[df_novelty['novelty_q100_percentile'] <= 10, 'novelty_category'] = 'Low'\n",
    "    \n",
    "    # Convert appln_id to string for merging\n",
    "    df_novelty['appln_id'] = df_novelty['appln_id'].astype(str)\n",
    "    df_tek_exploded['appln_id'] = df_tek_exploded['appln_id'].astype(str)\n",
    "    \n",
    "    # Merge with TEK funding data\n",
    "    df_tek_funding_novelty = pd.merge(\n",
    "        df_tek_exploded,\n",
    "        df_novelty[['appln_id', 'novelty_q100', 'novelty_category']],\n",
    "        on='appln_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"TEK funding with novelty: {len(df_tek_funding_novelty)} records\")\n",
    "    print(\"Novelty category distribution:\")\n",
    "    print(df_tek_funding_novelty['novelty_category'].value_counts())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Novelty data not found - will proceed without novelty analysis\")\n",
    "    df_tek_funding_novelty = df_tek_exploded.copy()\n",
    "    df_tek_funding_novelty['novelty_category'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79fbaf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorizing patents by novelty levels...\n",
      "Novelty categories distribution:\n",
      "novelty_category\n",
      "Medium    254491\n",
      "Low        31810\n",
      "High       31803\n",
      "Name: count, dtype: int64\n",
      "Percentages: {'Medium': 80.0, 'Low': 10.0, 'High': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Categorize patents by novelty levels (following the same logic as the GPU script)\n",
    "print(\"Categorizing patents by novelty levels...\")\n",
    "\n",
    "# Calculate percentiles for novelty_q100 among all patents with novelty scores\n",
    "df_novelty['novelty_q100_percentile'] = df_novelty['novelty_q100'].rank(pct=True, method='average') * 100\n",
    "\n",
    "# Classify patents into novelty categories based on percentiles\n",
    "df_novelty['novelty_category'] = 'Medium'  # Default\n",
    "df_novelty.loc[df_novelty['novelty_q100_percentile'] >= 90, 'novelty_category'] = 'High'\n",
    "df_novelty.loc[df_novelty['novelty_q100_percentile'] <= 10, 'novelty_category'] = 'Low'\n",
    "\n",
    "print(f\"Novelty categories distribution:\")\n",
    "novelty_counts = df_novelty['novelty_category'].value_counts()\n",
    "print(novelty_counts)\n",
    "print(f\"Percentages: {(novelty_counts / len(df_novelty) * 100).round(1).to_dict()}\")\n",
    "\n",
    "# Convert appln_id to string for consistent merging\n",
    "df_novelty['appln_id'] = df_novelty['appln_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a522d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging novelty data with company patent funding data...\n",
      "Company patents funding before novelty merge: 467532\n",
      "Unique patents in company funding data: 152084\n",
      "Patents in company funding data: 152084\n",
      "Patents with novelty scores: 318104\n",
      "Overlapping patents: 39311\n",
      "Overlap percentage: 25.8%\n",
      "Company patents funding after novelty merge: 156355\n",
      "Unique patents with novelty scores: 39311\n",
      "Unique companies with novelty-scored patents: 6563\n"
     ]
    }
   ],
   "source": [
    "# Merge novelty data with the existing company patent funding data\n",
    "print(\"Merging novelty data with company patent funding data...\")\n",
    "print(f\"Company patents funding before novelty merge: {len(df_company_patents_funding)}\")\n",
    "print(f\"Unique patents in company funding data: {df_company_patents_funding['appln_id'].nunique()}\")\n",
    "\n",
    "# Check overlap between datasets\n",
    "company_patent_ids = set(df_company_patents_funding['appln_id'].astype(str))\n",
    "novelty_patent_ids = set(df_novelty['appln_id'])\n",
    "overlap_ids = company_patent_ids.intersection(novelty_patent_ids)\n",
    "\n",
    "print(f\"Patents in company funding data: {len(company_patent_ids)}\")\n",
    "print(f\"Patents with novelty scores: {len(novelty_patent_ids)}\")\n",
    "print(f\"Overlapping patents: {len(overlap_ids)}\")\n",
    "print(f\"Overlap percentage: {len(overlap_ids)/len(company_patent_ids)*100:.1f}%\")\n",
    "\n",
    "# Merge with novelty data (inner join to keep only patents with novelty scores)\n",
    "df_company_patents_funding_novelty = pd.merge(\n",
    "    df_company_patents_funding,\n",
    "    df_novelty[['appln_id', 'novelty_q100', 'novelty_q90', 'novelty_q50', 'novelty_q100_percentile', 'novelty_category']],\n",
    "    on='appln_id',\n",
    "    how='inner'  # Only keep patents that have novelty scores\n",
    ")\n",
    "\n",
    "print(f\"Company patents funding after novelty merge: {len(df_company_patents_funding_novelty)}\")\n",
    "print(f\"Unique patents with novelty scores: {df_company_patents_funding_novelty['appln_id'].nunique()}\")\n",
    "print(f\"Unique companies with novelty-scored patents: {df_company_patents_funding_novelty['cb_uuid'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0e5cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUNDING ANALYSIS BY NOVELTY CATEGORIES ===\n",
      "Patent distribution by novelty category:\n",
      "novelty_category\n",
      "Medium    118584\n",
      "High       29684\n",
      "Low         8087\n",
      "Name: count, dtype: int64\n",
      "Percentages: {'Medium': 75.8, 'High': 19.0, 'Low': 5.2}\n",
      "\n",
      "=== FUNDING STATISTICS BY NOVELTY CATEGORY ===\n",
      "\n",
      "High Novelty Patents:\n",
      "  Total funding rounds: 25670\n",
      "  Unique companies: 1081\n",
      "  Unique patents: 5466\n",
      "  Total amount raised: $2,006,587,209,891.00\n",
      "  Average funding per round: $78,168,570.70\n",
      "  Median funding per round: $11,000,000.00\n",
      "\n",
      "Medium Novelty Patents:\n",
      "  Total funding rounds: 100023\n",
      "  Unique companies: 4999\n",
      "  Unique patents: 25780\n",
      "  Total amount raised: $40,049,765,411,050.00\n",
      "  Average funding per round: $400,405,560.83\n",
      "  Median funding per round: $8,713,627.00\n",
      "\n",
      "Low Novelty Patents:\n",
      "  Total funding rounds: 6789\n",
      "  Unique companies: 848\n",
      "  Unique patents: 2017\n",
      "  Total amount raised: $3,567,592,454,791.00\n",
      "  Average funding per round: $525,496,016.32\n",
      "  Median funding per round: $8,500,000.00\n"
     ]
    }
   ],
   "source": [
    "# Analyze funding patterns by novelty categories\n",
    "print(\"=== FUNDING ANALYSIS BY NOVELTY CATEGORIES ===\")\n",
    "\n",
    "# Distribution of patents by novelty category\n",
    "print(\"Patent distribution by novelty category:\")\n",
    "novelty_distribution = df_company_patents_funding_novelty['novelty_category'].value_counts()\n",
    "print(novelty_distribution)\n",
    "print(f\"Percentages: {(novelty_distribution / len(df_company_patents_funding_novelty) * 100).round(1).to_dict()}\")\n",
    "\n",
    "# Funding statistics by novelty category\n",
    "print(\"\\n=== FUNDING STATISTICS BY NOVELTY CATEGORY ===\")\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        (df_company_patents_funding_novelty['novelty_category'] == category) &\n",
    "        (df_company_patents_funding_novelty['raised_amount_usd'].notna())\n",
    "    ]\n",
    "    \n",
    "    if len(category_data) > 0:\n",
    "        print(f\"\\n{category} Novelty Patents:\")\n",
    "        print(f\"  Total funding rounds: {len(category_data)}\")\n",
    "        print(f\"  Unique companies: {category_data['cb_uuid'].nunique()}\")\n",
    "        print(f\"  Unique patents: {category_data['appln_id'].nunique()}\")\n",
    "        print(f\"  Total amount raised: ${category_data['raised_amount_usd'].sum():,.2f}\")\n",
    "        print(f\"  Average funding per round: ${category_data['raised_amount_usd'].mean():,.2f}\")\n",
    "        print(f\"  Median funding per round: ${category_data['raised_amount_usd'].median():,.2f}\")\n",
    "    else:\n",
    "        print(f\"\\n{category} Novelty Patents: No funding data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3096777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUNDING TYPE ANALYSIS BY NOVELTY CATEGORY ===\n",
      "\n",
      "High Novelty Patents - Top 5 Funding Types:\n",
      "  funding_round: 29684 (100.0%)\n",
      "\n",
      "Medium Novelty Patents - Top 5 Funding Types:\n",
      "  funding_round: 118584 (100.0%)\n",
      "\n",
      "Low Novelty Patents - Top 5 Funding Types:\n",
      "  funding_round: 8087 (100.0%)\n",
      "\n",
      "=== TEMPORAL FUNDING ANALYSIS BY NOVELTY CATEGORY ===\n",
      "\n",
      "High Novelty Patents - Top 5 Funding Years:\n",
      "  2021: 4580 funding rounds\n",
      "  2022: 2812 funding rounds\n",
      "  2020: 2749 funding rounds\n",
      "  2017: 2678 funding rounds\n",
      "  2013: 2668 funding rounds\n",
      "\n",
      "Medium Novelty Patents - Top 5 Funding Years:\n",
      "  2021: 14539 funding rounds\n",
      "  2017: 11298 funding rounds\n",
      "  2022: 10615 funding rounds\n",
      "  2013: 9949 funding rounds\n",
      "  2015: 9439 funding rounds\n",
      "\n",
      "Low Novelty Patents - Top 5 Funding Years:\n",
      "  2017: 919 funding rounds\n",
      "  2021: 915 funding rounds\n",
      "  2022: 725 funding rounds\n",
      "  2015: 642 funding rounds\n",
      "  2014: 628 funding rounds\n"
     ]
    }
   ],
   "source": [
    "# Analyze funding types by novelty category\n",
    "print(\"=== FUNDING TYPE ANALYSIS BY NOVELTY CATEGORY ===\")\n",
    "\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        (df_company_patents_funding_novelty['novelty_category'] == category) &\n",
    "        (df_company_patents_funding_novelty['type'].notna())\n",
    "    ]\n",
    "    \n",
    "    if len(category_data) > 0:\n",
    "        print(f\"\\n{category} Novelty Patents - Top 5 Funding Types:\")\n",
    "        funding_types = category_data['type'].value_counts().head(5)\n",
    "        for fund_type, count in funding_types.items():\n",
    "            percentage = (count / len(category_data)) * 100\n",
    "            print(f\"  {fund_type}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Temporal analysis by novelty category\n",
    "print(\"\\n=== TEMPORAL FUNDING ANALYSIS BY NOVELTY CATEGORY ===\")\n",
    "df_company_patents_funding_novelty['funding_year'] = pd.to_datetime(df_company_patents_funding_novelty['created_at']).dt.year\n",
    "\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        (df_company_patents_funding_novelty['novelty_category'] == category) &\n",
    "        (df_company_patents_funding_novelty['funding_year'].notna())\n",
    "    ]\n",
    "    \n",
    "    if len(category_data) > 0:\n",
    "        print(f\"\\n{category} Novelty Patents - Top 5 Funding Years:\")\n",
    "        yearly_funding = category_data['funding_year'].value_counts().head(5)\n",
    "        for year, count in yearly_funding.items():\n",
    "            print(f\"  {int(year)}: {count} funding rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f589145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARATIVE FUNDING ANALYSIS ===\n",
      "\n",
      "Summary by Novelty Category:\n",
      "Novelty_Category  Total_Patents  Unique_Companies  Unique_Patents  Funding_Rounds  Total_Funding_USD  Avg_Funding_USD  Median_Funding_USD\n",
      "            High          29684              1235            6250           25670   2006587209891.00      78168570.70         11000000.00\n",
      "          Medium         118584              5820           30626          100023  40049765411050.00     400405560.83          8713627.00\n",
      "             Low           8087               998            2435            6789   3567592454791.00     525496016.32          8500000.00\n",
      "\n",
      "=== FUNDING SUCCESS RATES BY NOVELTY ===\n",
      "High Novelty: 1081/1235 companies funded (87.5%)\n",
      "Medium Novelty: 4999/5820 companies funded (85.9%)\n",
      "Low Novelty: 848/998 companies funded (85.0%)\n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis across novelty categories\n",
    "print(\"=== COMPARATIVE FUNDING ANALYSIS ===\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        df_company_patents_funding_novelty['novelty_category'] == category\n",
    "    ]\n",
    "    \n",
    "    category_funding = category_data[category_data['raised_amount_usd'].notna()]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Novelty_Category': category,\n",
    "        'Total_Patents': len(category_data),\n",
    "        'Unique_Companies': category_data['cb_uuid'].nunique(),\n",
    "        'Unique_Patents': category_data['appln_id'].nunique(),\n",
    "        'Funding_Rounds': len(category_funding),\n",
    "        'Total_Funding_USD': category_funding['raised_amount_usd'].sum() if len(category_funding) > 0 else 0,\n",
    "        'Avg_Funding_USD': category_funding['raised_amount_usd'].mean() if len(category_funding) > 0 else 0,\n",
    "        'Median_Funding_USD': category_funding['raised_amount_usd'].median() if len(category_funding) > 0 else 0\n",
    "    })\n",
    "\n",
    "df_novelty_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary by Novelty Category:\")\n",
    "print(df_novelty_summary.to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Calculate funding success rates (companies with funding vs total companies)\n",
    "print(\"\\n=== FUNDING SUCCESS RATES BY NOVELTY ===\")\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        df_company_patents_funding_novelty['novelty_category'] == category\n",
    "    ]\n",
    "    \n",
    "    total_companies = category_data['cb_uuid'].nunique()\n",
    "    funded_companies = category_data[category_data['raised_amount_usd'].notna()]['cb_uuid'].nunique()\n",
    "    \n",
    "    if total_companies > 0:\n",
    "        success_rate = (funded_companies / total_companies) * 100\n",
    "        print(f\"{category} Novelty: {funded_companies}/{total_companies} companies funded ({success_rate:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{category} Novelty: No companies found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "666050a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STATISTICAL ANALYSIS: NOVELTY vs FUNDING ===\n",
      "Correlation between novelty scores and funding amounts:\n",
      "  novelty_q100 vs funding amount: -0.0354\n",
      "  novelty_q90 vs funding amount: -0.0245\n",
      "  novelty_q50 vs funding amount: -0.0081\n",
      "\n",
      "Kruskal-Wallis test for funding differences between novelty categories:\n",
      "  Test statistic: 471.2584\n",
      "  P-value: 0.000000\n",
      "  Groups tested: ['High', 'Medium', 'Low']\n",
      "  → Significant differences in funding amounts between novelty categories\n"
     ]
    }
   ],
   "source": [
    "# Statistical analysis of novelty vs funding relationships\n",
    "print(\"=== STATISTICAL ANALYSIS: NOVELTY vs FUNDING ===\")\n",
    "\n",
    "# Correlation between novelty scores and funding amounts\n",
    "funded_patents = df_company_patents_funding_novelty[\n",
    "    df_company_patents_funding_novelty['raised_amount_usd'].notna()\n",
    "].copy()\n",
    "\n",
    "if len(funded_patents) > 0:\n",
    "    print(\"Correlation between novelty scores and funding amounts:\")\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = {}\n",
    "    for novelty_col in ['novelty_q100', 'novelty_q90', 'novelty_q50']:\n",
    "        corr = funded_patents[novelty_col].corr(funded_patents['raised_amount_usd'])\n",
    "        correlations[novelty_col] = corr\n",
    "        print(f\"  {novelty_col} vs funding amount: {corr:.4f}\")\n",
    "    \n",
    "    # Test for significant differences in funding amounts between novelty categories\n",
    "    from scipy.stats import kruskal\n",
    "    \n",
    "    high_funding = funded_patents[funded_patents['novelty_category'] == 'High']['raised_amount_usd']\n",
    "    medium_funding = funded_patents[funded_patents['novelty_category'] == 'Medium']['raised_amount_usd']\n",
    "    low_funding = funded_patents[funded_patents['novelty_category'] == 'Low']['raised_amount_usd']\n",
    "    \n",
    "    # Remove empty groups for statistical testing\n",
    "    groups = [group for group in [high_funding, medium_funding, low_funding] if len(group) > 0]\n",
    "    group_names = [name for name, group in zip(['High', 'Medium', 'Low'], [high_funding, medium_funding, low_funding]) if len(group) > 0]\n",
    "    \n",
    "    if len(groups) >= 2:\n",
    "        statistic, p_value = kruskal(*groups)\n",
    "        print(f\"\\nKruskal-Wallis test for funding differences between novelty categories:\")\n",
    "        print(f\"  Test statistic: {statistic:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.6f}\")\n",
    "        print(f\"  Groups tested: {group_names}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"  → Significant differences in funding amounts between novelty categories\")\n",
    "        else:\n",
    "            print(\"  → No significant differences in funding amounts between novelty categories\")\n",
    "    else:\n",
    "        print(\"\\nInsufficient data for statistical testing\")\n",
    "else:\n",
    "    print(\"No funded patents available for statistical analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "636a87dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAVING NOVELTY-ENHANCED RESULTS ===\n",
      "Saved company patents with funding and novelty data: 156355 records\n",
      "Saved novelty funding summary to: /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/novelty_funding_summary.csv\n",
      "\n",
      "=== DETAILED BREAKDOWN BY NOVELTY CATEGORY ===\n",
      "\n",
      "High Novelty Patents:\n",
      "  File saved: /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/company_patents_funding_high_novelty.csv\n",
      "  Records: 29684\n",
      "  Unique companies: 1235\n",
      "  Unique patents: 6250\n",
      "  Total funding: $2,006,587,209,891.00\n",
      "  Companies with funding: 1081\n",
      "\n",
      "Medium Novelty Patents:\n",
      "  File saved: /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/company_patents_funding_medium_novelty.csv\n",
      "  Records: 118584\n",
      "  Unique companies: 5820\n",
      "  Unique patents: 30626\n",
      "  Total funding: $40,049,765,411,050.00\n",
      "  Companies with funding: 4999\n",
      "\n",
      "Low Novelty Patents:\n",
      "  File saved: /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/company_patents_funding_low_novelty.csv\n",
      "  Records: 8087\n",
      "  Unique companies: 998\n",
      "  Unique patents: 2435\n",
      "  Total funding: $3,567,592,454,791.00\n",
      "  Companies with funding: 848\n",
      "\n",
      "All novelty-enhanced files saved to: /home/thiesen/Documents/Projekt_EDV-TEK/AP 4 - Kommerzialisierung von TEKs/Matching Crunchbase and PATSTAT Data/final_results/\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY: PATENT NOVELTY AND FUNDING ANALYSIS\n",
      "============================================================\n",
      "Total patents with funding and novelty data: 156,355\n",
      "Original patents with funding data: 467,532\n",
      "Percentage with novelty scores: 33.4%\n",
      "High novelty patents: 29,684\n",
      "Medium novelty patents: 118,584\n",
      "Low novelty patents: 8,087\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the novelty-enhanced results\n",
    "print(\"=== SAVING NOVELTY-ENHANCED RESULTS ===\")\n",
    "\n",
    "# Save the complete dataset with novelty information\n",
    "novelty_output_file = f'{output_dir}company_patents_funding_with_novelty.csv'\n",
    "df_company_patents_funding_novelty.to_csv(novelty_output_file, index=False)\n",
    "print(f\"Saved company patents with funding and novelty data: {len(df_company_patents_funding_novelty)} records\")\n",
    "\n",
    "# Save the summary statistics\n",
    "novelty_summary_file = f'{output_dir}novelty_funding_summary.csv'\n",
    "df_novelty_summary.to_csv(novelty_summary_file, index=False)\n",
    "print(f\"Saved novelty funding summary to: {novelty_summary_file}\")\n",
    "\n",
    "# Create detailed breakdown by novelty category\n",
    "print(\"\\n=== DETAILED BREAKDOWN BY NOVELTY CATEGORY ===\")\n",
    "\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    category_data = df_company_patents_funding_novelty[\n",
    "        df_company_patents_funding_novelty['novelty_category'] == category\n",
    "    ]\n",
    "    \n",
    "    if len(category_data) > 0:\n",
    "        # Save category-specific data\n",
    "        category_file = f'{output_dir}company_patents_funding_{category.lower()}_novelty.csv'\n",
    "        category_data.to_csv(category_file, index=False)\n",
    "        \n",
    "        print(f\"\\n{category} Novelty Patents:\")\n",
    "        print(f\"  File saved: {category_file}\")\n",
    "        print(f\"  Records: {len(category_data)}\")\n",
    "        print(f\"  Unique companies: {category_data['cb_uuid'].nunique()}\")\n",
    "        print(f\"  Unique patents: {category_data['appln_id'].nunique()}\")\n",
    "        \n",
    "        # Funding summary for this category\n",
    "        category_funding = category_data[category_data['raised_amount_usd'].notna()]\n",
    "        if len(category_funding) > 0:\n",
    "            print(f\"  Total funding: ${category_funding['raised_amount_usd'].sum():,.2f}\")\n",
    "            print(f\"  Companies with funding: {category_funding['cb_uuid'].nunique()}\")\n",
    "\n",
    "print(f\"\\nAll novelty-enhanced files saved to: {output_dir}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY: PATENT NOVELTY AND FUNDING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total patents with funding and novelty data: {len(df_company_patents_funding_novelty):,}\")\n",
    "print(f\"Original patents with funding data: {len(df_company_patents_funding):,}\")\n",
    "print(f\"Percentage with novelty scores: {len(df_company_patents_funding_novelty)/len(df_company_patents_funding)*100:.1f}%\")\n",
    "print(f\"High novelty patents: {len(df_company_patents_funding_novelty[df_company_patents_funding_novelty['novelty_category'] == 'High']):,}\")\n",
    "print(f\"Medium novelty patents: {len(df_company_patents_funding_novelty[df_company_patents_funding_novelty['novelty_category'] == 'Medium']):,}\")\n",
    "print(f\"Low novelty patents: {len(df_company_patents_funding_novelty[df_company_patents_funding_novelty['novelty_category'] == 'Low']):,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4937d",
   "metadata": {},
   "source": [
    "# Temporal Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b6611b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funding year distribution:\n",
      "funding_year\n",
      "2014    9471\n",
      "2015    7310\n",
      "2016    7435\n",
      "2017    6395\n",
      "2018    7094\n",
      "2019    6464\n",
      "2020    5579\n",
      "2021    6453\n",
      "2022    7691\n",
      "2023    4399\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create temporal variables\n",
    "df_tek_funding_novelty['funding_date'] = pd.to_datetime(df_tek_funding_novelty['announced_on'])\n",
    "df_tek_funding_novelty['funding_year'] = df_tek_funding_novelty['funding_date'].dt.year\n",
    "df_tek_funding_novelty['funding_month'] = df_tek_funding_novelty['funding_date'].dt.month\n",
    "\n",
    "# Filter realistic years\n",
    "df_tek_funding_novelty = df_tek_funding_novelty[\n",
    "    (df_tek_funding_novelty['funding_year'] >= 2000) & \n",
    "    (df_tek_funding_novelty['funding_year'] <= 2024)\n",
    "]\n",
    "\n",
    "print(\"Funding year distribution:\")\n",
    "print(df_tek_funding_novelty['funding_year'].value_counts().sort_index().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ca42820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEK Categories Commercialization Analysis:\n",
      "            cb_uuid appln_id raised_amount_usd                       \\\n",
      "            nunique  nunique               sum          mean  count   \n",
      "y02_classes                                                           \n",
      "Y02A           1571     6856      1.126900e+12  3.888276e+07  28982   \n",
      "Y02B            755     3415      3.246363e+12  3.903286e+08   8317   \n",
      "Y02C             73      181      1.411143e+10  2.519898e+07    560   \n",
      "Y02D            946     5527      7.928480e+13  6.301447e+09  12582   \n",
      "Y02E           1838    12501      2.813509e+12  8.295765e+07  33915   \n",
      "Y02P           1236     4258      1.373439e+12  1.150573e+08  11937   \n",
      "Y02T            562     2228      3.682841e+11  7.228343e+07   5095   \n",
      "Y02W            202      410      3.904737e+10  4.437201e+07    880   \n",
      "\n",
      "            post_money_valuation_usd investor_count  \n",
      "                                mean           mean  \n",
      "y02_classes                                          \n",
      "Y02A                    2.103439e+09           2.37  \n",
      "Y02B                    1.498290e+09           2.25  \n",
      "Y02C                    8.738575e+08           1.96  \n",
      "Y02D                    1.774997e+09           3.42  \n",
      "Y02E                    1.682379e+09           2.13  \n",
      "Y02P                    1.431583e+09           2.51  \n",
      "Y02T                    2.056524e+09           1.98  \n",
      "Y02W                    1.563909e+09           2.03  \n"
     ]
    }
   ],
   "source": [
    "# Y02-specific commercialization analysis\n",
    "y02_analysis = df_tek_funding_novelty.groupby('y02_classes').agg({\n",
    "    'cb_uuid': 'nunique',  # Unique companies\n",
    "    'appln_id': 'nunique',  # Unique patents\n",
    "    'raised_amount_usd': ['sum', 'mean', 'count'],\n",
    "    'post_money_valuation_usd': 'mean',\n",
    "    'investor_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"TEK Categories Commercialization Analysis:\")\n",
    "print(y02_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "299cff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEK Categories Commercialization Analysis:\n",
      "            cb_uuid appln_id raised_amount_usd                       \\\n",
      "            nunique  nunique               sum          mean  count   \n",
      "y02_classes                                                           \n",
      "Y02A           1571     6856      1.126900e+12  3.888276e+07  28982   \n",
      "Y02B            755     3415      3.246363e+12  3.903286e+08   8317   \n",
      "Y02C             73      181      1.411143e+10  2.519898e+07    560   \n",
      "Y02D            946     5527      7.928480e+13  6.301447e+09  12582   \n",
      "Y02E           1838    12501      2.813509e+12  8.295765e+07  33915   \n",
      "Y02P           1236     4258      1.373439e+12  1.150573e+08  11937   \n",
      "Y02T            562     2228      3.682841e+11  7.228343e+07   5095   \n",
      "Y02W            202      410      3.904737e+10  4.437201e+07    880   \n",
      "\n",
      "            post_money_valuation_usd investor_count  \n",
      "                                mean           mean  \n",
      "y02_classes                                          \n",
      "Y02A                    2.103439e+09           2.37  \n",
      "Y02B                    1.498290e+09           2.25  \n",
      "Y02C                    8.738575e+08           1.96  \n",
      "Y02D                    1.774997e+09           3.42  \n",
      "Y02E                    1.682379e+09           2.13  \n",
      "Y02P                    1.431583e+09           2.51  \n",
      "Y02T                    2.056524e+09           1.98  \n",
      "Y02W                    1.563909e+09           2.03  \n"
     ]
    }
   ],
   "source": [
    "# Y02-specific commercialization analysis\n",
    "y02_analysis = df_tek_funding_novelty.groupby('y02_classes').agg({\n",
    "    'cb_uuid': 'nunique',  # Unique companies\n",
    "    'appln_id': 'nunique',  # Unique patents\n",
    "    'raised_amount_usd': ['sum', 'mean', 'count'],\n",
    "    'post_money_valuation_usd': 'mean',\n",
    "    'investor_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"TEK Categories Commercialization Analysis:\")\n",
    "print(y02_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a658a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Impact on Funding:\n",
      "                 cb_uuid raised_amount_usd                       \\\n",
      "                 nunique               sum          mean  count   \n",
      "novelty_category                                                  \n",
      "High                 608      1.248367e+12  1.151099e+08  10845   \n",
      "Low                  296      1.094402e+12  6.293285e+08   1739   \n",
      "Medium              2183      1.491639e+13  4.877664e+08  30581   \n",
      "\n",
      "                 post_money_valuation_usd  \n",
      "                                     mean  \n",
      "novelty_category                           \n",
      "High                         2.354424e+09  \n",
      "Low                          4.298818e+09  \n",
      "Medium                       1.530936e+09  \n"
     ]
    }
   ],
   "source": [
    "# Novelty vs funding success (only if novelty data available)\n",
    "if 'novelty_q100' in df_tek_funding_novelty.columns:\n",
    "    novelty_impact = df_tek_funding_novelty.groupby('novelty_category').agg({\n",
    "        'cb_uuid': 'nunique',\n",
    "        'raised_amount_usd': ['sum', 'mean', 'count'],\n",
    "        'post_money_valuation_usd': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"Novelty Impact on Funding:\")\n",
    "    print(novelty_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17e9a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Impact on Funding:\n",
      "                 cb_uuid raised_amount_usd                       \\\n",
      "                 nunique               sum          mean  count   \n",
      "novelty_category                                                  \n",
      "High                 608      1.248367e+12  1.151099e+08  10845   \n",
      "Low                  296      1.094402e+12  6.293285e+08   1739   \n",
      "Medium              2183      1.491639e+13  4.877664e+08  30581   \n",
      "\n",
      "                 post_money_valuation_usd  \n",
      "                                     mean  \n",
      "novelty_category                           \n",
      "High                         2.354424e+09  \n",
      "Low                          4.298818e+09  \n",
      "Medium                       1.530936e+09  \n"
     ]
    }
   ],
   "source": [
    "# Novelty vs funding success (only if novelty data available)\n",
    "if 'novelty_q100' in df_tek_funding_novelty.columns:\n",
    "    novelty_impact = df_tek_funding_novelty.groupby('novelty_category').agg({\n",
    "        'cb_uuid': 'nunique',\n",
    "        'raised_amount_usd': ['sum', 'mean', 'count'],\n",
    "        'post_money_valuation_usd': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"Novelty Impact on Funding:\")\n",
    "    print(novelty_impact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edv_tek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
